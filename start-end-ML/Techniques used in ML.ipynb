{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db300f4d",
   "metadata": {},
   "source": [
    "#  Feature Scaling\n",
    "\n",
    "Scaling features in the context of machine learning refers to the process of transforming your input variables (features) so that they all have the same scale or range. This is important for several reasons, and there are different methods to achieve feature scaling.\n",
    "\n",
    "#### Why Scaling Features is Necessary:\n",
    "\n",
    "**Different Scales:** Features in your dataset may have different units of measurement or different scales. For example, one feature might measure weight in kilograms, while another measures height in meters. These different scales can cause some machine learning algorithms to perform poorly because they may give too much importance to features with larger scales.\n",
    "\n",
    "**Gradient Descent:** Many machine learning algorithms, especially those based on gradient descent (e.g., linear regression, neural networks), converge faster when features are on similar scales. If features have significantly different scales, the gradient descent process can be skewed, leading to longer convergence times or convergence to suboptimal solutions.\n",
    "\n",
    "**Distance-Based Algorithms:** Algorithms that rely on distances between data points, such as k-means clustering or support vector machines, can be sensitive to feature scales. Features with larger scales might dominate the distance calculations, leading to biased results.\n",
    "\n",
    "**Regularization:** Regularization techniques, like L1 and L2 regularization, can be sensitive to feature scales. In some cases, regularization might unfairly penalize features with larger scales.\n",
    "\n",
    "#### How to Scale Features:\n",
    "\n",
    "There are several common methods for scaling features:\n",
    "\n",
    "**Min-Max Scaling (Normalization):** This method scales features to a specific range, usually [0, 1]. It subtracts the minimum value of the feature from each data point and then divides by the range (the difference between the maximum and minimum values). Min-max scaling preserves the relative relationships between data points.\n",
    "\n",
    "Formula for Min-Max Scaling:\n",
    "\n",
    "`X_scaled = (X - X_min) / (X_max - X_min)`\n",
    "\n",
    "**Standardization (Z-score Scaling):** Standardization transforms features to have a mean of 0 and a standard deviation of 1. It subtracts the mean of the feature from each data point and then divides by the standard deviation. Standardization is less sensitive to outliers compared to min-max scaling.\n",
    "\n",
    "Formula for Standardization:\n",
    "\n",
    "`X_scaled = (X - X_mean) / X_std`\n",
    "\n",
    "**Robust Scaling:** This method is similar to standardization but uses the median and the interquartile range (IQR) instead of the mean and standard deviation. It's less sensitive to outliers because it relies on the median and IQR, which are robust to extreme values.\n",
    "\n",
    "Formula for Robust Scaling:\n",
    "\n",
    "`X_scaled = (X - X_median) / IQR`\n",
    "\n",
    "The choice of scaling method depends on your specific dataset and the requirements of your machine learning algorithm. It's common practice to try different scaling methods and evaluate their impact on your model's performance. \n",
    "\n",
    "Some algorithms, like **decision trees and random forests, are not sensitive to feature scales and may not require feature scaling**. However, for many other algorithms, scaling features is an important preprocessing step to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbedd2c",
   "metadata": {},
   "source": [
    "Feature scaling is primarily applied to the feature matrix (X) because it helps algorithms that rely on distance or gradient-based optimization converge more quickly and perform better when features are on a similar scale. \n",
    "\n",
    "Typically we do not need to scale the target variable (y) in most machine learning scenarios. The target variable represents the quantity you are trying to predict, and its scale is an inherent part of the problem you are trying to solve. Scaling the target variable doesn't usually provide any meaningful benefits and can even lead to confusion or incorrect interpretations of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deec4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets say we have X feature\n",
    "\n",
    "X=[[1,2,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65604e3",
   "metadata": {},
   "source": [
    "**It's common practice to try different scaling methods and evaluate their impact on your model's performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e02807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd06fe66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_X = scaler.fit_transform(X)\n",
    "standardized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a8b74b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0ae6d",
   "metadata": {},
   "source": [
    "**To train a model and make predictions on the given DataFrame while performing feature scaling, you can follow these steps. We'll use Python with scikit-learn for this task:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66663d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.852155027526014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.DataFrame({\n",
    "    'Car': ['Toyota', 'Mitsubishi', 'Skoda', 'Fiat', 'Mini', 'VW', 'Skoda', 'Mercedes', 'Ford', 'Audi'],\n",
    "    'Model': ['Aygo', 'Space Star', 'Citigo', '500', 'Cooper', 'Up!', 'Fabia', 'A-Class', 'Fiesta', 'A1'],\n",
    "    'Volume': [1.0, 1.2, 1.0, 0.9, 1.5, 1.0, 1.4, 1.5, 1.5, 1.6],\n",
    "    'Weight': [790, 1160, 929, 865, 1140, 929, 1109, 1365, 1112, 1150],\n",
    "    'CO2': [99, 95, 95, 90, 105, 105, 90, 92, 98, 99]\n",
    "})\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['Volume', 'Weight']]\n",
    "y = data['CO2']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature scaling (standardization) on the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fd91f",
   "metadata": {},
   "source": [
    "In machine learning, **it's a common practice to perform feature scaling on the independent variables (X) rather than the dependent variable (y)**. Feature scaling helps ensure that all the features are on a similar scale, which can be important for many machine learning algorithms. Scaling the dependent variable (y) is typically not necessary, as it's the target variable you're trying to predict, and scaling it may not have a meaningful impact on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97805b50",
   "metadata": {},
   "source": [
    "In the code I provided, we use `fit_transform()` on the training set and `transform()` on the test set for feature scaling. This approach is a common practice in machine learning, and here's why we do it this way:\n",
    "\n",
    "**fit_transform() on Training Data:**\n",
    "\n",
    "When you apply feature scaling to the training data, you use fit_transform(). This step computes and stores the scaling parameters (mean and standard deviation in the case of StandardScaler) based on the training data.\n",
    "\n",
    "The purpose of this step is to ensure that the scaling parameters are determined solely from the training data and not influenced by the test data.\n",
    "\n",
    "It scales the training data and returns the scaled version.\n",
    "\n",
    "**transform() on Test Data:**\n",
    "\n",
    "After the scaling parameters have been determined from the training data using fit_transform(), you use transform() on the test data.\n",
    "\n",
    "The scaling parameters (mean and standard deviation) computed from the training data are then applied to the test data.\n",
    "This ensures that the test data is scaled using the same parameters as the training data, maintaining consistency and preventing data leakage from the test set into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ef9ab",
   "metadata": {},
   "source": [
    "The approach of using `fit_transform()` on the training data and `transform()` on the test data applies to other scaling methods like `MinMaxScaler` and `RobustScaler` as well. This approach is consistent across various scaling techniques to maintain data integrity and ensure that the scaling parameters are determined based on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07408e7c",
   "metadata": {},
   "source": [
    "# Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d7f78",
   "metadata": {},
   "source": [
    "Handling categorical data is a crucial step in preparing your data for machine learning models, as most machine learning algorithms require numerical input. Here are different ways to handle categorical data:\n",
    "\n",
    "**One-Hot Encoding (Dummy Variables):**\n",
    "\n",
    "One-hot encoding is a common method for converting categorical variables into numerical format.\n",
    "\n",
    "It creates binary columns for each category and assigns a 1 or 0 to indicate the presence or absence of each category.\n",
    "\n",
    "One-hot encoding is suitable for nominal categorical variables (categories without inherent order).\n",
    "Example:\n",
    "\n",
    "<pre>\n",
    "| Color |\n",
    "|-------|\n",
    "| Red   |\n",
    "| Blue  |\n",
    "| Green |\n",
    "\n",
    "After one-hot encoding:\n",
    "| Color_Red | Color_Blue | Color_Green |\n",
    "|-----------|------------|-------------|\n",
    "| 1         | 0          | 0           |\n",
    "| 0         | 1          | 0           |\n",
    "| 0         | 0          | 1           |\n",
    "</pre>\n",
    "\n",
    "**Label Encoding:**\n",
    "\n",
    "Label encoding assigns a unique integer value to each category in a categorical variable.\n",
    "\n",
    "It is suitable for ordinal categorical variables (categories with a specific order).\n",
    "\n",
    "Be cautious when using label encoding with nominal variables, as it may imply an ordinal relationship that doesn't exist.\n",
    "Example:\n",
    "\n",
    "<pre>\n",
    "| Size   |\n",
    "|--------|\n",
    "| Small  |\n",
    "| Medium |\n",
    "| Large  |\n",
    "\n",
    "After label encoding:\n",
    "| Size  |\n",
    "|-------|\n",
    "| 0     |\n",
    "| 1     |\n",
    "| 2     |\n",
    "</pre>\n",
    "\n",
    "**Frequency Encoding (Count Encoding):**\n",
    "\n",
    "Frequency encoding replaces each category with its frequency or count in the dataset.\n",
    "\n",
    "It can capture the importance of each category based on its prevalence in the data.\n",
    "Example:\n",
    "\n",
    "<pre>\n",
    "| Country |\n",
    "|---------|\n",
    "| USA     |\n",
    "| Japan   |\n",
    "| USA     |\n",
    "| Germany |\n",
    "| Japan   |\n",
    "\n",
    "After frequency encoding:\n",
    "| Country |\n",
    "|---------|\n",
    "| 2       |\n",
    "| 2       |\n",
    "| 2       |\n",
    "| 1       |\n",
    "| 2       |\n",
    "</pre>\n",
    "\n",
    "**Target Encoding (Mean Encoding):**\n",
    "\n",
    "Target encoding calculates the mean of the target variable for each category and replaces the category with its corresponding mean.\n",
    "\n",
    "It can be useful when the categorical variable has a strong relationship with the target variable.\n",
    "Example:\n",
    "\n",
    "<pre>\n",
    "| City   | Target |\n",
    "|--------|--------|\n",
    "| NYC    | 25     |\n",
    "| Boston | 20     |\n",
    "| NYC    | 25     |\n",
    "| LA     | 18     |\n",
    "| Boston | 20     |\n",
    "\n",
    "After target encoding:\n",
    "| City   | Target |\n",
    "|--------|--------|\n",
    "| 23.3   | 23.3   |\n",
    "| 20.0   | 20.0   |\n",
    "| 23.3   | 23.3   |\n",
    "| 19.0   | 19.0   |\n",
    "| 20.0   | 20.0   |\n",
    "</pre>\n",
    "\n",
    "**Binary Encoding:**\n",
    "\n",
    "Binary encoding converts each category into binary code and creates separate binary columns for each digit in the code.\n",
    "It can be more efficient than one-hot encoding when dealing with a large number of categories.\n",
    "Example:\n",
    "\n",
    "<pre>\n",
    "| Gender |\n",
    "|--------|\n",
    "| Male   |\n",
    "| Female |\n",
    "| Male   |\n",
    "| Other  |\n",
    "\n",
    "After binary encoding:\n",
    "| Gender_0 | Gender_1 | Gender_2 |\n",
    "|----------|----------|----------|\n",
    "| 0        | 1        | 0        |\n",
    "| 1        | 0        | 1        |\n",
    "| 0        | 1        | 0        |\n",
    "| 1        | 1        | 0        |\n",
    "</pre>\n",
    "\n",
    "**Embedding Layers (for Neural Networks):**\n",
    "\n",
    "In deep learning and neural networks, you can use embedding layers to learn representations of categorical variables.\n",
    "Embedding layers map each category to a lower-dimensional vector space where relationships between categories can be learned during model training.\n",
    "\n",
    "\n",
    "These are some common techniques for handling categorical data, and the choice of method depends on the nature of the data and the requirements of your machine learning model. It's essential to select the appropriate method to avoid introducing bias or misrepresenting the information contained in your categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01124683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0           0            0          1\n",
      "1           1            0          0\n",
      "2           0            1          0\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
    "\n",
    "# Use pandas' get_dummies() function for one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(data, columns=['Color'])\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47523e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Size_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Size  Size_encoded\n",
       "0   Small             2\n",
       "1  Medium             1\n",
       "2   Large             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoding:\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.DataFrame({'Size': ['Small', 'Medium', 'Large']})\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the 'Size' column\n",
    "data['Size_encoded'] = label_encoder.fit_transform(data['Size'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5cf611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Country_encoded\n",
      "0      USA                2\n",
      "1    Japan                2\n",
      "2      USA                2\n",
      "3  Germany                1\n",
      "4    Japan                2\n"
     ]
    }
   ],
   "source": [
    "# Frequency Encoding (Count Encoding):\n",
    "\n",
    "data = pd.DataFrame({'Country': ['USA', 'Japan', 'USA', 'Germany', 'Japan']})\n",
    "\n",
    "# Use value_counts() to create a frequency encoding dictionary\n",
    "frequency_encoding = data['Country'].value_counts().to_dict()\n",
    "\n",
    "# Map the frequency encoding to the 'Country' column\n",
    "data['Country_encoded'] = data['Country'].map(frequency_encoding)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d0f491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     City  Target  City_encoded\n",
      "0     NYC      25          25.0\n",
      "1  Boston      20          20.0\n",
      "2     NYC      25          25.0\n",
      "3      LA      18          18.0\n",
      "4  Boston      20          20.0\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding (Mean Encoding):\n",
    "\n",
    "data = pd.DataFrame({'City': ['NYC', 'Boston', 'NYC', 'LA', 'Boston'], 'Target': [25, 20, 25, 18, 20]})\n",
    "\n",
    "# Calculate the mean encoding for each category in 'City'\n",
    "mean_encoding = data.groupby('City')['Target'].mean().to_dict()\n",
    "\n",
    "# Map the mean encoding to the 'City' column\n",
    "data['City_encoded'] = data['City'].map(mean_encoding)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender_0  Gender_1\n",
      "0         0         1\n",
      "1         1         0\n",
      "2         0         1\n",
      "3         1         1\n"
     ]
    }
   ],
   "source": [
    "# Binary Encoding:\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({'Gender': ['Male', 'Female', 'Male', 'Other']})\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "binary_encoder = ce.BinaryEncoder(cols=['Gender'])\n",
    "\n",
    "# Apply binary encoding to the 'Gender' column\n",
    "binary_encoded = binary_encoder.fit_transform(data)\n",
    "print(binary_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc6980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc671d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41ba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2c8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629561b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5073f632",
   "metadata": {},
   "source": [
    "https://medium.com/geekculture/how-to-handle-categorical-variables-7c1ee198c55c#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjZmNzI1NDEwMWY1NmU0MWNmMzVjOTkyNmRlODRhMmQ1NTJiNGM2ZjEiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDU4NjAxMzc4MDc0MzU5MDI0NDUiLCJlbWFpbCI6InNhdXJhYmhwODUwNzBAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5NTQwMjE2MiwibmFtZSI6IlNhdXJhYmggUHJha2FzaCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NJWnlscWwwRWxBRWxNcDFHVUl2VndKUDUyUjQzY0RuRUc0Q3BsS3JEYi1tQT1zOTYtYyIsImdpdmVuX25hbWUiOiJTYXVyYWJoIiwiZmFtaWx5X25hbWUiOiJQcmFrYXNoIiwibG9jYWxlIjoiZW4iLCJpYXQiOjE2OTU0MDI0NjIsImV4cCI6MTY5NTQwNjA2MiwianRpIjoiNmJjYTQ0M2RlYTM5MzAwODJlYWFjODRkYzliYjM1MzMzMzdjM2E0ZiJ9.eE2DQPbFgl8bhqgO6cV0a2GUP3ER0M_X7v6ff3AS9CByrK8NxF5Ov4aP925uYw4n9kC_QKnqT6g8Ay60aOvJx6IoXtA7w-nEU2WT56W5VT-7Cf0h_6W8uwv-64kjb8y8BtoNcoV75SAI5Qaf-N8iOGqr08DgXS9ilxgfaoOuByh_gWzWVxu5sZ-qyBT5m5oOIca0-zRKe3f0yKmU7dTV6Ax06npdwF22vJAvlxcaczMmiiueckL-500z5JUlmWDGHqdz16qn3sxMQe1KasrDvdGDBur9DIohgUdo2Onritu-E4VlUMKHxFnU-2OmhQcBTKXF4I79zJYfzcuzPtmOyA\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/big-data-center-of-excellence/how-to-handle-categorical-values-byaryan-a769b2bb4361\n",
    "\n",
    "https://towardsdatascience.com/handling-categorical-data-the-right-way-9d1279956fc6\n",
    "\n",
    "https://www.datacamp.com/tutorial/categorical-data\n",
    "\n",
    "https://dev.to/anurag629/transforming-categorical-data-a-practical-guide-to-handling-non-numerical-variables-for-machine-learning-algorithms-cld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c35ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd724b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b7dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b727f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4980f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e27b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45e9b8e2",
   "metadata": {},
   "source": [
    "**Handling categorical data can be done both before and after data visualization, and the choice depends on your specific goals and the nature of your data. Here are some considerations for when to handle categorical data in relation to data visualization:**\n",
    "\n",
    "1. Before Data Visualization:\n",
    "\n",
    "Data Preparation: It's common to handle categorical data before visualization as part of the data preprocessing step. This includes tasks like one-hot encoding, label encoding, or any other method to convert categorical variables into a numerical format.\n",
    "\n",
    "Visualization Clarity: By preprocessing categorical data before visualization, you can create visualizations that are more interpretable and informative. For example, one-hot encoding can make it easier to create bar charts or other plots that show the distribution of categories.\n",
    "\n",
    "Dimension Reduction: Some visualization techniques may require dimensionality reduction methods like Principal Component Analysis (PCA) or t-SNE. Handling categorical data before such techniques can be necessary to avoid issues associated with mixed data types.\n",
    "\n",
    "Data Exploration: Categorical encoding can help you understand the relationships between categorical variables and the target variable or among themselves, which can guide the choice of visualizations.\n",
    "\n",
    "2. After Data Visualization:\n",
    "\n",
    "Exploratory Data Analysis (EDA): In some cases, you might initially perform data visualization to explore the dataset and understand the relationships between variables. After EDA, you can decide how to handle categorical data based on insights gained from visualizations.\n",
    "\n",
    "Visualization for Encoding Choices: Visualization can help you make informed decisions about how to encode categorical variables. For example, you may discover that one-hot encoding is appropriate for variables with few unique categories, while other encoding methods may be better suited for variables with many categories.\n",
    "\n",
    "Visualizing Encoding Impact: You might want to visualize the impact of different encoding methods on your data, such as comparing the distribution of a target variable before and after encoding.\n",
    "\n",
    "In summary, there is no strict rule about whether to handle categorical data before or after data visualization. The choice depends on your analysis workflow and objectives. However, it's common to start with some data preprocessing before visualization to ensure that the visualizations accurately represent the data and that you can derive meaningful insights from them. Subsequent visualizations may then inform further data preprocessing decisions. Ultimately, the goal is to strike a balance between data preparation and exploration to gain a deep understanding of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f4067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393e4f5a",
   "metadata": {},
   "source": [
    "**When you have a dataset that contains a mix of categorical and numerical features, and you want to apply feature scaling to the numerical features, a common approach is to use a combination of techniques, such as one-hot encoding for categorical features and feature scaling for numerical features. Here's a step-by-step approach:**\n",
    "\n",
    "Identify Categorical and Numerical Features:\n",
    "\n",
    "Start by identifying which features in your dataset are categorical (nominal or ordinal) and which are numerical (continuous or discrete).\n",
    "Preprocess Categorical Features (One-Hot Encoding):\n",
    "\n",
    "For categorical features, you can use one-hot encoding to convert them into a numerical format that machine learning algorithms can work with.\n",
    "One-hot encoding creates binary columns for each category within a categorical feature. Each binary column represents the presence or absence of a category for a particular data point.\n",
    "Example: If you have a \"Color\" feature with categories [\"Red\", \"Green\", \"Blue\"], one-hot encoding would create three binary columns, one for each color.\n",
    "Preprocess Numerical Features (Feature Scaling):\n",
    "\n",
    "For numerical features, it's a good practice to apply feature scaling to ensure that all numerical features have a similar scale. Common scaling techniques include:\n",
    "Standardization (Z-score scaling): Scales the data to have a mean of 0 and a standard deviation of 1. It's suitable for features that are approximately normally distributed.\n",
    "Min-Max Scaling (Normalization): Scales the data to a specific range, typically between 0 and 1. It's suitable when the feature distribution does not follow a normal distribution and you want to preserve the original data's relationships.\n",
    "Robust Scaling: Scales data based on the median and interquartile range, making it robust to outliers.\n",
    "Concatenate Scaled Numerical Features and Encoded Categorical Features:\n",
    "\n",
    "After scaling the numerical features and one-hot encoding the categorical features, you can concatenate these processed features into a single feature matrix.\n",
    "Model Training and Evaluation:\n",
    "\n",
    "You can now use this processed dataset for training your machine learning model. Ensure that you split your data into training and testing sets for model evaluation.\n",
    "Here's an example of how you can implement this approach using Python and libraries like Pandas and Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa564b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Category1', 'Category2']\n",
    "numerical_features = ['Numerical1', 'Numerical2']\n",
    "\n",
    "# Create transformers for preprocessing\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # You can use other scalers as needed\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Define your model and combine it with preprocessing using a Pipeline\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the preprocessing and model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5819e0",
   "metadata": {},
   "source": [
    "we used `Pipeline()` and `ColumnTransformer()` for structured and efficient data preprocessing. Here's why each of these tools is valuable:\n",
    "\n",
    "`Pipeline()`:\n",
    "\n",
    "- A Pipeline is a way to streamline a lot of the routine processes by combining multiple data preprocessing steps and a machine learning model into a single, easy-to-use object.\n",
    "- It helps ensure that the steps of data preprocessing are applied in the correct order. In machine learning workflows, it's common to first preprocess data (e.g., scaling and encoding) and then apply a model. A Pipeline enforces this sequence.\n",
    "- It simplifies the code and reduces the risk of data leakage. For example, when you scale your data, you should fit the scaler on the training data and then transform both the training and testing data. A Pipeline automates this process.\n",
    "- Makes it easier to cross-validate your model. When using cross-validation techniques, a Pipeline ensures that preprocessing steps are applied consistently to each fold of the data.\n",
    "\n",
    "`ColumnTransformer()`:\n",
    "\n",
    "- A ColumnTransformer is particularly useful when you have a dataset with different types of features (e.g., categorical and numerical) that require different preprocessing techniques.\n",
    "- It allows you to apply different preprocessing pipelines to different subsets of columns in your dataset.\n",
    "- In the example given, we used ColumnTransformer to apply one-hot encoding to the categorical features and standard scaling to the numerical features.\n",
    "- This approach keeps the code organized and makes it clear which preprocessing steps apply to which types of features.\n",
    "\n",
    "In summary, using Pipeline and ColumnTransformer together offers a structured and efficient way to preprocess data with multiple feature types, ensuring that the correct transformations are applied to each feature subset while simplifying the code and making it more maintainable. This is especially helpful in machine learning workflows where consistency and repeatability are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f164c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09dc48b",
   "metadata": {},
   "source": [
    "**You can certainly use an approach where you first separate the categorical and numerical features into two dataframes, apply feature scaling to the numerical dataframe (df2), and one-hot encoding to the categorical dataframe (df1). Afterward, you can combine them back into a single dataframe (df) for model training and evaluation. This approach can be conceptually easier to follow and maintain.**\n",
    "\n",
    "Here's how you can implement this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ffad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Category1', 'Category2']\n",
    "numerical_features = ['Numerical1', 'Numerical2']\n",
    "\n",
    "# Create dataframes for categorical and numerical features\n",
    "df1 = df[categorical_features]\n",
    "df2 = df[numerical_features]\n",
    "\n",
    "# Apply preprocessing to df1 (categorical features)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "df1_processed = categorical_transformer.fit_transform(df1)\n",
    "\n",
    "# Apply preprocessing to df2 (numerical features)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # You can use other scalers as needed\n",
    "])\n",
    "df2_processed = numerical_transformer.fit_transform(df2)\n",
    "\n",
    "# Combine df1 and df2 back into a single dataframe (df)\n",
    "df_processed = pd.concat([pd.DataFrame(df1_processed.toarray()), pd.DataFrame(df2_processed, columns=numerical_features)], axis=1)\n",
    "\n",
    "# Define your model and combine it with preprocessing using a Pipeline\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df_processed\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the preprocessing and model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d11db0",
   "metadata": {},
   "source": [
    "**Certainly, you can achieve the same preprocessing without using Pipeline and keep it simple. You can separately apply feature scaling to numerical features and one-hot encoding to categorical features, and then combine them into a single dataframe. Here's how you can do it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Category1', 'Category2']\n",
    "numerical_features = ['Numerical1', 'Numerical2']\n",
    "\n",
    "# Create dataframes for categorical and numerical features\n",
    "df1 = df[categorical_features]\n",
    "df2 = df[numerical_features]\n",
    "\n",
    "# Apply one-hot encoding to df1 (categorical features)\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "df1_encoded = encoder.fit_transform(df1)\n",
    "\n",
    "# Apply feature scaling to df2 (numerical features)\n",
    "scaler = StandardScaler()\n",
    "df2_scaled = scaler.fit_transform(df2)\n",
    "\n",
    "# Combine df1_encoded and df2_scaled back into a single dataframe (df)\n",
    "df_processed = pd.concat([pd.DataFrame(df1_encoded.toarray()), pd.DataFrame(df2_scaled, columns=numerical_features)], axis=1)\n",
    "\n",
    "# Define your model and use df_processed for training and evaluation\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df_processed\n",
    "y = df['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cd6a4",
   "metadata": {},
   "source": [
    "**The choice between using a Pipeline or not depends on your specific needs, preferences, and the complexity of your data preprocessing workflow. Both approaches are valid and have their advantages and disadvantages:**\n",
    "\n",
    "`With Pipeline`:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Structured Workflow: Using a Pipeline enforces a structured and sequential workflow for data preprocessing. This can make your code more organized and easier to follow.\n",
    "\n",
    "- Data Leakage Prevention: A Pipeline helps prevent data leakage by ensuring that preprocessing steps are applied consistently to both the training and testing datasets.\n",
    "\n",
    "- Cross-Validation: When using cross-validation techniques, a Pipeline ensures that preprocessing is part of each fold, making cross-validation results more reliable.\n",
    "\n",
    "- Reusability: Pipelines are reusable components. Once you define a pipeline, you can use it on different datasets or with different models with minimal code changes.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Complexity: For simple preprocessing tasks, a Pipeline might be overkill and add unnecessary complexity to your code.\n",
    "\n",
    "`Without Pipeline`:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Simplicity: Without a Pipeline, your code can be simpler and easier to understand, especially for straightforward preprocessing tasks.\n",
    "\n",
    "- Control: You have more fine-grained control over each preprocessing step, which can be beneficial if you need to customize or modify individual steps.\n",
    "\n",
    "- Transparency: It's easier to see and modify each preprocessing step separately.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Potential for Data Leakage: Without a Pipeline, you must be careful to apply preprocessing consistently to both training and testing data to avoid data leakage.\n",
    "\n",
    "- Maintenance: As your preprocessing steps grow more complex, the code may become harder to maintain and extend.\n",
    "\n",
    "In conclusion, the best approach depends on the complexity of your data and your specific requirements. For simple preprocessing tasks, a straightforward approach without a Pipeline can be more concise and easier to work with. However, for more complex workflows, or when working on projects where data consistency and reproducibility are critical, using a Pipeline is often recommended. Ultimately, the choice between the two approaches should align with the goals of your data preprocessing and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7d890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e827ba1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be7f1d3",
   "metadata": {},
   "source": [
    "# Statisitical Analysis  \n",
    "\n",
    "- Statistical analysis involves the collection, organization, analysis, interpretation, and presentation of data. \n",
    "- It aims to uncover patterns, trends, relationships, and insights within the data. \n",
    "- Statistical analysis employs various statistical techniques and methods to summarize and describe data, make inferences about populations based on samples, and test hypotheses. \n",
    "- It provides a framework for making informed decisions and predictions based on data evidence. \n",
    "- Statistical analysis is crucial in extracting meaningful information from data and understanding the underlying phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ecc2e",
   "metadata": {},
   "source": [
    "# Statistical modelling\n",
    "\n",
    "- Statistical modeling is a process of building mathematical models to represent and understand the relationships between variables in data. \n",
    "- These models are constructed based on statistical principles and techniques. \n",
    "- Statistical models can be descriptive, predictive, or inferential in nature. \n",
    "        - Descriptive models summarize the characteristics of data. \n",
    "        - Predictive models aim to forecast or estimate future outcomes. \n",
    "        - Inferential models are used to make inferences or draw conclusions about populations based on sample data. \n",
    "- Statistical modeling involves selecting an appropriate model, estimating model parameters from data, evaluating model performance, and making interpretations or predictions based on the model results. \n",
    "- It plays a fundamental role in various fields such as economics, finance, biology, social sciences, and many others, providing insights into complex phenomena and aiding decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea9ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00db412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f20b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddbb2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20cbc817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.38535265e+00,  3.28414053e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.50652052e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.24920112e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-5.37177559e-01,  1.93979142e+00, -1.16971425e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.50652052e+00,  7.88807586e-01, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.02184904e+00,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.74885626e+00, -3.62176246e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-5.37177559e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00,  7.88807586e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-1.87002413e+00, -1.31979479e-01, -1.51073881e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-5.25060772e-02,  2.16998818e+00, -1.45390138e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.73673948e-01,  3.09077525e+00, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-5.37177559e-01,  1.93979142e+00, -1.39706395e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.73673948e-01,  1.70959465e+00, -1.16971425e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.28338910e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-5.37177559e-01,  7.88807586e-01, -1.16971425e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.50652052e+00,  1.24920112e+00, -1.56757623e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  5.58610819e-01, -1.16971425e+00,\n",
       "        -9.20547742e-01],\n",
       "       [-1.26418478e+00,  7.88807586e-01, -1.05603939e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00, -1.31979479e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  7.88807586e-01, -1.22655167e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-7.79513300e-01,  1.01900435e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-7.79513300e-01,  7.88807586e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.38535265e+00,  3.28414053e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00,  9.82172869e-02, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-5.37177559e-01,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-7.79513300e-01,  2.40018495e+00, -1.28338910e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-4.16009689e-01,  2.63038172e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  3.28414053e-01, -1.45390138e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-4.16009689e-01,  1.01900435e+00, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  1.24920112e+00, -1.34022653e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-1.74885626e+00, -1.31979479e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.01900435e+00, -1.39706395e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.62768839e+00, -1.74335684e+00, -1.39706395e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.74885626e+00,  3.28414053e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.01900435e+00, -1.22655167e+00,\n",
       "        -7.88915558e-01],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.05603939e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.50652052e+00,  3.28414053e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-6.58345429e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  5.58610819e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [ 1.40150837e+00,  3.28414053e-01,  5.35408562e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 6.74501145e-01,  3.28414053e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  6.49083415e-01,\n",
       "         3.95774101e-01],\n",
       "       [-4.16009689e-01, -1.74335684e+00,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 7.95669016e-01, -5.92373012e-01,  4.78571135e-01,\n",
       "         3.95774101e-01],\n",
       "       [-1.73673948e-01, -5.92373012e-01,  4.21733708e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01,  5.58610819e-01,  5.35408562e-01,\n",
       "         5.27406285e-01],\n",
       "       [-1.14301691e+00, -1.51316008e+00, -2.60315415e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 9.16836886e-01, -3.62176246e-01,  4.78571135e-01,\n",
       "         1.32509732e-01],\n",
       "       [-7.79513300e-01, -8.22569778e-01,  8.07091462e-02,\n",
       "         2.64141916e-01],\n",
       "       [-1.02184904e+00, -2.43394714e+00, -1.46640561e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 6.86617933e-02, -1.31979479e-01,  2.51221427e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.89829664e-01, -1.97355361e+00,  1.37546573e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 3.10997534e-01, -3.62176246e-01,  5.35408562e-01,\n",
       "         2.64141916e-01],\n",
       "       [-2.94841818e-01, -3.62176246e-01, -8.98031345e-02,\n",
       "         1.32509732e-01],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  3.64896281e-01,\n",
       "         2.64141916e-01],\n",
       "       [-2.94841818e-01, -1.31979479e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  1.94384000e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 4.32165405e-01, -1.97355361e+00,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-2.94841818e-01, -1.28296331e+00,  8.07091462e-02,\n",
       "        -1.30754636e-01],\n",
       "       [ 6.86617933e-02,  3.28414053e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 3.10997534e-01, -5.92373012e-01,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01, -1.28296331e+00,  6.49083415e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 3.10997534e-01, -5.92373012e-01,  5.35408562e-01,\n",
       "         8.77547895e-04],\n",
       "       [ 6.74501145e-01, -3.62176246e-01,  3.08058854e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 9.16836886e-01, -1.31979479e-01,  3.64896281e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 1.15917263e+00, -5.92373012e-01,  5.92245988e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 1.03800476e+00, -1.31979479e-01,  7.05920842e-01,\n",
       "         6.59038469e-01],\n",
       "       [ 1.89829664e-01, -3.62176246e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-1.73673948e-01, -1.05276654e+00, -1.46640561e-01,\n",
       "        -2.62386821e-01],\n",
       "       [-4.16009689e-01, -1.51316008e+00,  2.38717193e-02,\n",
       "        -1.30754636e-01],\n",
       "       [-4.16009689e-01, -1.51316008e+00, -3.29657076e-02,\n",
       "        -2.62386821e-01],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  8.07091462e-02,\n",
       "         8.77547895e-04],\n",
       "       [ 1.89829664e-01, -8.22569778e-01,  7.62758269e-01,\n",
       "         5.27406285e-01],\n",
       "       [-5.37177559e-01, -1.31979479e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.89829664e-01,  7.88807586e-01,  4.21733708e-01,\n",
       "         5.27406285e-01],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  5.35408562e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 5.53333275e-01, -1.74335684e+00,  3.64896281e-01,\n",
       "         1.32509732e-01],\n",
       "       [-2.94841818e-01, -1.31979479e-01,  1.94384000e-01,\n",
       "         1.32509732e-01],\n",
       "       [-4.16009689e-01, -1.28296331e+00,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [-4.16009689e-01, -1.05276654e+00,  3.64896281e-01,\n",
       "         8.77547895e-04],\n",
       "       [ 3.10997534e-01, -1.31979479e-01,  4.78571135e-01,\n",
       "         2.64141916e-01],\n",
       "       [-5.25060772e-02, -1.05276654e+00,  1.37546573e-01,\n",
       "         8.77547895e-04],\n",
       "       [-1.02184904e+00, -1.74335684e+00, -2.60315415e-01,\n",
       "        -2.62386821e-01],\n",
       "       [-2.94841818e-01, -8.22569778e-01,  2.51221427e-01,\n",
       "         1.32509732e-01],\n",
       "       [-1.73673948e-01, -1.31979479e-01,  2.51221427e-01,\n",
       "         8.77547895e-04],\n",
       "       [-1.73673948e-01, -3.62176246e-01,  2.51221427e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 4.32165405e-01, -3.62176246e-01,  3.08058854e-01,\n",
       "         1.32509732e-01],\n",
       "       [-9.00681170e-01, -1.28296331e+00, -4.30827696e-01,\n",
       "        -1.30754636e-01],\n",
       "       [-1.73673948e-01, -5.92373012e-01,  1.94384000e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01,  5.58610819e-01,  1.27429511e+00,\n",
       "         1.71209594e+00],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.52267624e+00, -1.31979479e-01,  1.21745768e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 5.53333275e-01, -3.62176246e-01,  1.04694540e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  1.16062026e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 2.12851559e+00, -1.31979479e-01,  1.61531967e+00,\n",
       "         1.18556721e+00],\n",
       "       [-1.14301691e+00, -1.28296331e+00,  4.21733708e-01,\n",
       "         6.59038469e-01],\n",
       "       [ 1.76501198e+00, -3.62176246e-01,  1.44480739e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 1.03800476e+00, -1.28296331e+00,  1.16062026e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 1.64384411e+00,  1.24920112e+00,  1.33113254e+00,\n",
       "         1.71209594e+00],\n",
       "       [ 7.95669016e-01,  3.28414053e-01,  7.62758269e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 6.74501145e-01, -8.22569778e-01,  8.76433123e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.15917263e+00, -1.31979479e-01,  9.90107977e-01,\n",
       "         1.18556721e+00],\n",
       "       [-1.73673948e-01, -1.28296331e+00,  7.05920842e-01,\n",
       "         1.05393502e+00],\n",
       "       [-5.25060772e-02, -5.92373012e-01,  7.62758269e-01,\n",
       "         1.58046376e+00],\n",
       "       [ 6.74501145e-01,  3.28414053e-01,  8.76433123e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  9.90107977e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 2.24968346e+00,  1.70959465e+00,  1.67215710e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 2.24968346e+00, -1.05276654e+00,  1.78583195e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 1.89829664e-01, -1.97355361e+00,  7.05920842e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.28034050e+00,  3.28414053e-01,  1.10378283e+00,\n",
       "         1.44883158e+00],\n",
       "       [-2.94841818e-01, -5.92373012e-01,  6.49083415e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 2.24968346e+00, -5.92373012e-01,  1.67215710e+00,\n",
       "         1.05393502e+00],\n",
       "       [ 5.53333275e-01, -8.22569778e-01,  6.49083415e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 1.64384411e+00,  3.28414053e-01,  1.27429511e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 4.32165405e-01, -5.92373012e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 3.10997534e-01, -1.31979479e-01,  6.49083415e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 1.64384411e+00, -1.31979479e-01,  1.16062026e+00,\n",
       "         5.27406285e-01],\n",
       "       [ 1.88617985e+00, -5.92373012e-01,  1.33113254e+00,\n",
       "         9.22302838e-01],\n",
       "       [ 2.49201920e+00,  1.70959465e+00,  1.50164482e+00,\n",
       "         1.05393502e+00],\n",
       "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 5.53333275e-01, -5.92373012e-01,  7.62758269e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 3.10997534e-01, -1.05276654e+00,  1.04694540e+00,\n",
       "         2.64141916e-01],\n",
       "       [ 2.24968346e+00, -1.31979479e-01,  1.33113254e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 5.53333275e-01,  7.88807586e-01,  1.04694540e+00,\n",
       "         1.58046376e+00],\n",
       "       [ 6.74501145e-01,  9.82172869e-02,  9.90107977e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.89829664e-01, -1.31979479e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  9.33270550e-01,\n",
       "         1.18556721e+00],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  1.04694540e+00,\n",
       "         1.58046376e+00],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  7.62758269e-01,\n",
       "         1.44883158e+00],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.15917263e+00,  3.28414053e-01,  1.21745768e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
       "         1.71209594e+00],\n",
       "       [ 1.03800476e+00, -1.31979479e-01,  8.19595696e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 5.53333275e-01, -1.28296331e+00,  7.05920842e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  8.19595696e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 4.32165405e-01,  7.88807586e-01,  9.33270550e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 6.86617933e-02, -1.31979479e-01,  7.62758269e-01,\n",
       "         7.90670654e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47483daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26470281,  0.4800266 ],\n",
       "       [-2.08096115, -0.67413356],\n",
       "       [-2.36422905, -0.34190802],\n",
       "       [-2.29938422, -0.59739451],\n",
       "       [-2.38984217,  0.64683538],\n",
       "       [-2.07563095,  1.48917752],\n",
       "       [-2.44402884,  0.0476442 ],\n",
       "       [-2.23284716,  0.22314807],\n",
       "       [-2.33464048, -1.11532768],\n",
       "       [-2.18432817, -0.46901356],\n",
       "       [-2.1663101 ,  1.04369065],\n",
       "       [-2.32613087,  0.13307834],\n",
       "       [-2.2184509 , -0.72867617],\n",
       "       [-2.6331007 , -0.96150673],\n",
       "       [-2.1987406 ,  1.86005711],\n",
       "       [-2.26221453,  2.68628449],\n",
       "       [-2.2075877 ,  1.48360936],\n",
       "       [-2.19034951,  0.48883832],\n",
       "       [-1.898572  ,  1.40501879],\n",
       "       [-2.34336905,  1.12784938],\n",
       "       [-1.914323  ,  0.40885571],\n",
       "       [-2.20701284,  0.92412143],\n",
       "       [-2.7743447 ,  0.45834367],\n",
       "       [-1.81866953,  0.08555853],\n",
       "       [-2.22716331,  0.13725446],\n",
       "       [-1.95184633, -0.62561859],\n",
       "       [-2.05115137,  0.24216355],\n",
       "       [-2.16857717,  0.52714953],\n",
       "       [-2.13956345,  0.31321781],\n",
       "       [-2.26526149, -0.3377319 ],\n",
       "       [-2.14012214, -0.50454069],\n",
       "       [-1.83159477,  0.42369507],\n",
       "       [-2.61494794,  1.79357586],\n",
       "       [-2.44617739,  2.15072788],\n",
       "       [-2.10997488, -0.46020184],\n",
       "       [-2.2078089 , -0.2061074 ],\n",
       "       [-2.04514621,  0.66155811],\n",
       "       [-2.52733191,  0.59229277],\n",
       "       [-2.42963258, -0.90418004],\n",
       "       [-2.16971071,  0.26887896],\n",
       "       [-2.28647514,  0.44171539],\n",
       "       [-1.85812246, -2.33741516],\n",
       "       [-2.5536384 , -0.47910069],\n",
       "       [-1.96444768,  0.47232667],\n",
       "       [-2.13705901,  1.14222926],\n",
       "       [-2.0697443 , -0.71105273],\n",
       "       [-2.38473317,  1.1204297 ],\n",
       "       [-2.39437631, -0.38624687],\n",
       "       [-2.22944655,  0.99795976],\n",
       "       [-2.20383344,  0.00921636],\n",
       "       [ 1.10178118,  0.86297242],\n",
       "       [ 0.73133743,  0.59461473],\n",
       "       [ 1.24097932,  0.61629765],\n",
       "       [ 0.40748306, -1.75440399],\n",
       "       [ 1.0754747 , -0.20842105],\n",
       "       [ 0.38868734, -0.59328364],\n",
       "       [ 0.74652974,  0.77301931],\n",
       "       [-0.48732274, -1.85242909],\n",
       "       [ 0.92790164,  0.03222608],\n",
       "       [ 0.01142619, -1.03401828],\n",
       "       [-0.11019628, -2.65407282],\n",
       "       [ 0.44069345, -0.06329519],\n",
       "       [ 0.56210831, -1.76472438],\n",
       "       [ 0.71956189, -0.18622461],\n",
       "       [-0.0333547 , -0.43900321],\n",
       "       [ 0.87540719,  0.50906396],\n",
       "       [ 0.35025167, -0.19631173],\n",
       "       [ 0.15881005, -0.79209574],\n",
       "       [ 1.22509363, -1.6222438 ],\n",
       "       [ 0.1649179 , -1.30260923],\n",
       "       [ 0.73768265,  0.39657156],\n",
       "       [ 0.47628719, -0.41732028],\n",
       "       [ 1.2341781 , -0.93332573],\n",
       "       [ 0.6328582 , -0.41638772],\n",
       "       [ 0.70266118, -0.06341182],\n",
       "       [ 0.87427365,  0.25079339],\n",
       "       [ 1.25650912, -0.07725602],\n",
       "       [ 1.35840512,  0.33131168],\n",
       "       [ 0.66480037, -0.22592785],\n",
       "       [-0.04025861, -1.05871855],\n",
       "       [ 0.13079518, -1.56227183],\n",
       "       [ 0.02345269, -1.57247559],\n",
       "       [ 0.24153827, -0.77725638],\n",
       "       [ 1.06109461, -0.63384324],\n",
       "       [ 0.22397877, -0.28777351],\n",
       "       [ 0.42913912,  0.84558224],\n",
       "       [ 1.04872805,  0.5220518 ],\n",
       "       [ 1.04453138, -1.38298872],\n",
       "       [ 0.06958832, -0.21950333],\n",
       "       [ 0.28347724, -1.32932464],\n",
       "       [ 0.27907778, -1.12002852],\n",
       "       [ 0.62456979,  0.02492303],\n",
       "       [ 0.33653037, -0.98840402],\n",
       "       [-0.36218338, -2.01923787],\n",
       "       [ 0.28858624, -0.85573032],\n",
       "       [ 0.09136066, -0.18119213],\n",
       "       [ 0.22771687, -0.38492008],\n",
       "       [ 0.57638829, -0.1548736 ],\n",
       "       [-0.44766702, -1.54379203],\n",
       "       [ 0.25673059, -0.5988518 ],\n",
       "       [ 1.84456887,  0.87042131],\n",
       "       [ 1.15788161, -0.69886986],\n",
       "       [ 2.20526679,  0.56201048],\n",
       "       [ 1.44015066, -0.04698759],\n",
       "       [ 1.86781222,  0.29504482],\n",
       "       [ 2.75187334,  0.8004092 ],\n",
       "       [ 0.36701769, -1.56150289],\n",
       "       [ 2.30243944,  0.42006558],\n",
       "       [ 2.00668647, -0.71143865],\n",
       "       [ 2.25977735,  1.92101038],\n",
       "       [ 1.36417549,  0.69275645],\n",
       "       [ 1.60267867, -0.42170045],\n",
       "       [ 1.8839007 ,  0.41924965],\n",
       "       [ 1.2601151 , -1.16226042],\n",
       "       [ 1.4676452 , -0.44227159],\n",
       "       [ 1.59007732,  0.67624481],\n",
       "       [ 1.47143146,  0.25562182],\n",
       "       [ 2.42632899,  2.55666125],\n",
       "       [ 3.31069558,  0.01778095],\n",
       "       [ 1.26376667, -1.70674538],\n",
       "       [ 2.0377163 ,  0.91046741],\n",
       "       [ 0.97798073, -0.57176432],\n",
       "       [ 2.89765149,  0.41364106],\n",
       "       [ 1.33323218, -0.48181122],\n",
       "       [ 1.7007339 ,  1.01392187],\n",
       "       [ 1.95432671,  1.0077776 ],\n",
       "       [ 1.17510363, -0.31639447],\n",
       "       [ 1.02095055,  0.06434603],\n",
       "       [ 1.78834992, -0.18736121],\n",
       "       [ 1.86364755,  0.56229073],\n",
       "       [ 2.43595373,  0.25928443],\n",
       "       [ 2.30492772,  2.62632347],\n",
       "       [ 1.86270322, -0.17854949],\n",
       "       [ 1.11414774, -0.29292262],\n",
       "       [ 1.2024733 , -0.81131527],\n",
       "       [ 2.79877045,  0.85680333],\n",
       "       [ 1.57625591,  1.06858111],\n",
       "       [ 1.3462921 ,  0.42243061],\n",
       "       [ 0.92482492,  0.0172231 ],\n",
       "       [ 1.85204505,  0.67612817],\n",
       "       [ 2.01481043,  0.61388564],\n",
       "       [ 1.90178409,  0.68957549],\n",
       "       [ 1.15788161, -0.69886986],\n",
       "       [ 2.04055823,  0.8675206 ],\n",
       "       [ 1.9981471 ,  1.04916875],\n",
       "       [ 1.87050329,  0.38696608],\n",
       "       [ 1.56458048, -0.89668681],\n",
       "       [ 1.5211705 ,  0.26906914],\n",
       "       [ 1.37278779,  1.01125442],\n",
       "       [ 0.96065603, -0.02433167]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e424be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870503</td>\n",
       "      <td>0.386966</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.564580</td>\n",
       "      <td>-0.896687</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.521170</td>\n",
       "      <td>0.269069</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.372788</td>\n",
       "      <td>1.011254</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.960656</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PC1       PC2  Target\n",
       "0   -2.264703  0.480027       0\n",
       "1   -2.080961 -0.674134       0\n",
       "2   -2.364229 -0.341908       0\n",
       "3   -2.299384 -0.597395       0\n",
       "4   -2.389842  0.646835       0\n",
       "..        ...       ...     ...\n",
       "145  1.870503  0.386966       2\n",
       "146  1.564580 -0.896687       2\n",
       "147  1.521170  0.269069       2\n",
       "148  1.372788  1.011254       2\n",
       "149  0.960656 -0.024332       2\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e47dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.72962445 0.22850762]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "targets = np.unique(y)\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets, colors):\n",
    "    indices_to_keep = pca_df['Target'] == target\n",
    "    plt.scatter(pca_df.loc[indices_to_keep, 'PC1'],\n",
    "                pca_df.loc[indices_to_keep, 'PC2'],\n",
    "                c=color,\n",
    "                label=target)\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.legend(targets)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
