{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f189d5d5-27f4-48e4-9492-86d2712c85dd",
   "metadata": {},
   "source": [
    "# creating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbb5e13-5a48-4729-805c-013a3b9acd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Programs\\\\spark\\\\spark-3.5.3-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a99c73-f476-40ac-b8f0-fff16a94dd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-SRU23534:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>learnPySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19675aa9d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"learnPySpark\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7400340b-c8b1-44d3-9776-5477d651cd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da676ca-b78c-4509-bc12-ddfbbc31db06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_conf',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'active',\n",
       " 'addArtifact',\n",
       " 'addArtifacts',\n",
       " 'addTag',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'clearTags',\n",
       " 'client',\n",
       " 'conf',\n",
       " 'copyFromLocalToFs',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'getTags',\n",
       " 'interruptAll',\n",
       " 'interruptOperation',\n",
       " 'interruptTag',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'removeTag',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'udtf',\n",
       " 'version']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6014101f-d527-4b25-99f9-ffe8e2cb3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None. The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "    +-----+---+\n",
      "    |   _1| _2|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries.\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  1|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Create a DataFrame with column names specified.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create an empty DataFrame.\n",
      "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "    as the DataFrame lacks data from which the schema can be inferred.\n",
      "    \n",
      "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    +----+---+\n",
      "    \n",
      "    Create a DataFrame from Row objects.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "    >>> df.show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    +---+---+\n",
      "    |  0|  1|\n",
      "    +---+---+\n",
      "    |  1|  2|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5359984-8bc8-4793-a766-f347ae495f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"manish\"), (2, \"ramesh\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=['id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc558d87-3ba9-4cfd-a7e9-c6739a526c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f9a01f-10fe-4f96-b00b-53f8b87b176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e992ccf-fcba-4007-b0b0-fb5d7503be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n",
      "\n",
      "class StructType(DataType)\n",
      " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
      " |  \n",
      " |  Struct type, consisting of a list of :class:`StructField`.\n",
      " |  \n",
      " |  This is the data type representing a :class:`Row`.\n",
      " |  \n",
      " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
      " |  A contained :class:`StructField` can be accessed by its name or position.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.sql.types import *\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1[\"f1\"]\n",
      " |  StructField('f1', StringType(), True)\n",
      " |  >>> struct1[0]\n",
      " |  StructField('f1', StringType(), True)\n",
      " |  \n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
      " |  >>> struct1 == struct2\n",
      " |  False\n",
      " |  \n",
      " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
      " |  using class:`StructType` and class:`StructField`:\n",
      " |  \n",
      " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
      " |  >>> schema = StructType([\n",
      " |  ...     StructField(\"name\", StringType()),\n",
      " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
      " |  ... ])\n",
      " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
      " |  >>> df.printSchema()\n",
      " |  root\n",
      " |   |-- name: string (nullable = true)\n",
      " |   |-- languagesSkills: array (nullable = true)\n",
      " |   |    |-- element: string (containsNull = true)\n",
      " |  >>> df.show()\n",
      " |  +-----+---------------+\n",
      " |  | name|languagesSkills|\n",
      " |  +-----+---------------+\n",
      " |  |Alice|  [Java, Scala]|\n",
      " |  |  Bob|[Python, Scala]|\n",
      " |  +-----+---------------+\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StructType\n",
      " |      DataType\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
      " |      Access fields by name or slice.\n",
      " |  \n",
      " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
      " |      Iterate the fields\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Return the number of fields.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
      " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
      " |      The method accepts either:\n",
      " |      \n",
      " |          a) A single parameter which is a :class:`StructField` object.\n",
      " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      " |             metadata(optional). The data_type parameter may be either a String or a\n",
      " |             :class:`DataType` object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : str or :class:`StructField`\n",
      " |          Either the name of the field or a :class:`StructField` object\n",
      " |      data_type : :class:`DataType`, optional\n",
      " |          If present, the DataType of the :class:`StructField` to create\n",
      " |      nullable : bool, optional\n",
      " |          Whether the field to add should be nullable (default True)\n",
      " |      metadata : dict, optional\n",
      " |          Any additional metadata (default None)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
      " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |  \n",
      " |  fieldNames(self) -> List[str]\n",
      " |      Returns all field names in a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
      " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct.fieldNames()\n",
      " |      ['f1']\n",
      " |  \n",
      " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
      " |      Converts an internal SQL object into a native Python object.\n",
      " |  \n",
      " |  jsonValue(self) -> Dict[str, Any]\n",
      " |  \n",
      " |  needConversion(self) -> bool\n",
      " |      Does this type needs conversion between Python object and internal SQL object.\n",
      " |      \n",
      " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      " |  \n",
      " |  simpleString(self) -> str\n",
      " |  \n",
      " |  toInternal(self, obj: Tuple) -> Tuple\n",
      " |      Converts a Python object into an internal SQL object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
      " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
      " |      \n",
      " |      Below is a JSON schema it must adhere to::\n",
      " |      \n",
      " |          {\n",
      " |            \"title\":\"StructType\",\n",
      " |            \"description\":\"Schema of StructType in json format\",\n",
      " |            \"type\":\"object\",\n",
      " |            \"properties\":{\n",
      " |               \"fields\":{\n",
      " |                  \"description\":\"Array of struct fields\",\n",
      " |                  \"type\":\"array\",\n",
      " |                  \"items\":{\n",
      " |                      \"type\":\"object\",\n",
      " |                      \"properties\":{\n",
      " |                         \"name\":{\n",
      " |                            \"description\":\"Name of the field\",\n",
      " |                            \"type\":\"string\"\n",
      " |                         },\n",
      " |                         \"type\":{\n",
      " |                            \"description\": \"Type of the field. Can either be\n",
      " |                                            another nested StructType or primitive type\",\n",
      " |                            \"type\":\"object/string\"\n",
      " |                         },\n",
      " |                         \"nullable\":{\n",
      " |                            \"description\":\"If nulls are allowed\",\n",
      " |                            \"type\":\"boolean\"\n",
      " |                         },\n",
      " |                         \"metadata\":{\n",
      " |                            \"description\":\"Additional metadata to supply\",\n",
      " |                            \"type\":\"object\"\n",
      " |                         },\n",
      " |                         \"required\":[\n",
      " |                            \"name\",\n",
      " |                            \"type\",\n",
      " |                            \"nullable\",\n",
      " |                            \"metadata\"\n",
      " |                         ]\n",
      " |                      }\n",
      " |                 }\n",
      " |              }\n",
      " |           }\n",
      " |         }\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      json : dict or a dict-like object e.g. JSON object\n",
      " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
      " |          each of which must have specific keys (name, type, nullable, metadata).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> json_str = '''\n",
      " |      ...  {\n",
      " |      ...      \"fields\": [\n",
      " |      ...          {\n",
      " |      ...              \"metadata\": {},\n",
      " |      ...              \"name\": \"Person\",\n",
      " |      ...              \"nullable\": true,\n",
      " |      ...              \"type\": {\n",
      " |      ...                  \"fields\": [\n",
      " |      ...                      {\n",
      " |      ...                          \"metadata\": {},\n",
      " |      ...                          \"name\": \"name\",\n",
      " |      ...                          \"nullable\": false,\n",
      " |      ...                          \"type\": \"string\"\n",
      " |      ...                      },\n",
      " |      ...                      {\n",
      " |      ...                          \"metadata\": {},\n",
      " |      ...                          \"name\": \"surname\",\n",
      " |      ...                          \"nullable\": false,\n",
      " |      ...                          \"type\": \"string\"\n",
      " |      ...                      }\n",
      " |      ...                  ],\n",
      " |      ...                  \"type\": \"struct\"\n",
      " |      ...              }\n",
      " |      ...          }\n",
      " |      ...      ],\n",
      " |      ...      \"type\": \"struct\"\n",
      " |      ...  }\n",
      " |      ...  '''\n",
      " |      >>> import json\n",
      " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
      " |      >>> scheme.simpleString()\n",
      " |      'struct<Person:struct<name:string,surname:string>>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataType:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self) -> int\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ne__(self, other: Any) -> bool\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  json(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from DataType:\n",
      " |  \n",
      " |  typeName() -> str from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataType:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "help(StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba61059-e61f-4f1b-994d-1b6b5d794f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, \"manish\"), (2, \"ramesh\")]\n",
    "\n",
    "schema = StructType([StructField(name=\"id\", dataType=IntegerType()),\n",
    "           StructField(name=\"name\", dataType=StringType())])\n",
    "\n",
    "type(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecfb639-d659-4515-9ff0-d2faa8eb67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae1192e-424e-4560-8f39-dcdc43600991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1cc8146-75d5-4e19-b379-cae7b2fff65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa2d1e2-5f72-497c-9bc3-fd925d1df055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|mahesh|\n",
      "|  2|suresh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{'id':1, 'name':\"mahesh\"},\n",
    "       {'id':2, 'name':\"suresh\"}]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe97170-f914-4288-b6cb-77b32f561e66",
   "metadata": {},
   "source": [
    "# reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c424dba-dd88-4ddb-ac78-f180ee0fbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path=\"50_Startups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e42347-a2d1-4118-b0d2-feee15aceb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df) # display dataframe in databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a28eff1c-bcd6-412c-b928-3ff9ba4b7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|      _c0|           _c1|            _c2|       _c3|      _c4|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # display dataframe in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b175f9-4584-47a4-954b-3bde14c4057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd21a5d3-91ad-49dc-9cce-d53244f7bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|              0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path=\"50_Startups.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3beb160a-e688-494f-b57a-61a5dcf6b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: string (nullable = true)\n",
      " |-- Administration: string (nullable = true)\n",
      " |-- Marketing Spend: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae785a51-db6c-4b30-93a5-c72a7517e819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|Profit|\n",
      "+---------+--------------+---------------+----------+------+\n",
      "|   165349|        136898|         471784|  New York|192262|\n",
      "|   162598|        151378|         443899|California|191792|\n",
      "|   153442|        101146|         407935|   Florida|191050|\n",
      "|   144372|        118672|         383200|  New York|182902|\n",
      "|   142107|         91392|         366168|   Florida|166188|\n",
      "|   131877|         99815|         362861|  New York|156991|\n",
      "|   134615|        147199|         127717|California|156123|\n",
      "|   130298|        145530|         323877|   Florida|155753|\n",
      "|   120543|        148719|         311613|  New York|152212|\n",
      "|   123335|        108679|         304982|California|149760|\n",
      "|   101913|        110594|         229161|   Florida|146122|\n",
      "|   100672|         91791|         249745|California|144259|\n",
      "|    93864|        127320|         249839|   Florida|141586|\n",
      "|    91992|        135495|         252665|California|134307|\n",
      "|   119943|        156547|         256513|   Florida|132603|\n",
      "|   114524|        122617|         261776|  New York|129917|\n",
      "|    78013|        121598|         264346|California|126993|\n",
      "|    94657|        145078|         282574|  New York|125370|\n",
      "|    91749|        114176|         294920|   Florida|124267|\n",
      "|    86420|        153514|              0|  New York|122777|\n",
      "+---------+--------------+---------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding datatype\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(field='R&D Spend', data_type=DecimalType())\\\n",
    "                    .add(field='Administration', data_type=DecimalType())\\\n",
    "                    .add(field='Marketing Spend', data_type=DecimalType())\\\n",
    "                    .add(field='State', data_type=StringType())\\\n",
    "                    .add(field='Profit', data_type=DecimalType()) \n",
    "\n",
    "df = spark.read.csv(path=\"50_Startups.csv\", schema=schema, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52e98ada-75f2-4eaa-a3b8-5ddc36834f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: decimal(10,0) (nullable = true)\n",
      " |-- Administration: decimal(10,0) (nullable = true)\n",
      " |-- Marketing Spend: decimal(10,0) (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: decimal(10,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b7fd7a-0f82-41ab-a8f2-c9f38118f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# automatic type detection\n",
    "df = spark.read.csv(path=\"50_Startups.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85722435-eb48-4e86-b25b-e346940881df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddbfe87-5f9e-4af2-bc06-61956dbe8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|              0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another method\n",
    "df = spark.read.format('csv').option(key='header', value=True).load(path=\"50_Startups.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9abf4fd5-be68-42e0-95e8-1a2da6ea9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|              0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading multiple files: similar schema then only work\n",
    "df = spark.read.csv(path=[\"50_Startups.csv\"], header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19df5249-fcb7-43f7-9ca0-f19cedd550f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading all files of folder: similar schema then only work\n",
    "df = spark.read.csv(path=\"files/*.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877a2c7-7a08-4786-aa7e-2219e4940b69",
   "metadata": {},
   "source": [
    "# write dataframe into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c21b481d-0e64-45d2-89f5-42d98f1a2e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\"), (2, \"ramesh\")]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3460baf5-48b7-46d0-af62-4e622c026ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(path=\"files\", header=True) # method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3c08a9f-a3cb-4384-831b-44bcd4f291cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(path=\"files\", header=True, mode='ignore') # method 1 mode=ignore, error(deafult), append, overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1b4fde4-e241-4fcf-9d02-f1d5f2aab8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.options(header=True, delimeter=',').csv('files2') # method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "455dd477-e045-406f-bd60-a7da617c5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True).csv('files3') # method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8add64d7-9eeb-4cab-81ab-818cb7ef15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(path=\"files\", header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8ea0f-964a-450d-a040-dc8e4db8659e",
   "metadata": {},
   "source": [
    "# reading json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce8a35c0-7427-47b8-860e-c0fb1360a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path=\"sample1.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30f9307b-8547-4dc5-9583-2144ee3a5311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d023399-7c46-4c28-a82f-b678376ac8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(field='id', data_type=IntegerType())\\\n",
    "                    .add(field='name', data_type=StringType())\n",
    "\n",
    "df = spark.read.json(path=\"sample1.json\", schema=schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "924433bb-7dc3-422e-9d5d-3c712b320b0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# ERROR\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path=\"sample2.json\") # ERROR\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9409554-da34-408d-8daa-d18d0eb68cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "|  4|  David|\n",
      "|  5|    Eve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path=\"sample2.json\", multiLine=True) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3dd98b9-0df0-40dc-b430-61f74e357f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading multiple files\n",
    "df = spark.read.json(path=[\"sample1.json\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd37e0b4-8200-449c-ae09-8d8c0218f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading all files in folder\n",
    "df = spark.read.json(path=\"json_files/*.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffadd19-d4da-4fed-b41e-2d9bb99d7931",
   "metadata": {},
   "source": [
    "# write dataframe into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "218e8540-3c69-488b-bea1-9d2133e546f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\"), (2, \"ramesh\")]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf2b2164-8d77-40ed-ac4d-d8b6cd6938be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(path=\"json_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55db4f76-b932-439d-91b5-297256d5516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(path=\"json_files\", mode='ignore') # error(default), append, overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25306c7b-98d0-478a-97d8-ddb3c54969d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path=\"json_files\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fd4b2-4621-4939-872f-14cabb1c74cc",
   "metadata": {},
   "source": [
    "# reading parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dd27e38-7d1c-47c9-bcf6-4113c55b2713",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet_sample.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ERROR will be generated\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    542\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(path=\"parquet_sample.parquet\")\n",
    "df.show()\n",
    "\n",
    "# ERROR will be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4adde-7ab8-409e-b715-13dc0f41b896",
   "metadata": {},
   "source": [
    "# write dataframe into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14bbc827-f55c-480e-9835-64b7b16ee1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|manish|\n",
      "|  2|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\"), (2, \"ramesh\")]\n",
    "schema = ['id', 'name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ad6c1d8-8f59-476f-886c-8a5db5505b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(path=\"parquet_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c250c-a7d8-4dcf-bfa8-b9327d2f7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path=\"parquet_files/*.parquet\")\n",
    "df.show()\n",
    "\n",
    "# ERROR will be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b749dc9-ef3e-4276-b36c-499dafc0c60f",
   "metadata": {},
   "source": [
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfc99fe5-89fa-4eb5-ad77-4249ffc9df3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            comments|\n",
      "+---+--------------------+\n",
      "|  1|gvdchzsvdvduydgiv...|\n",
      "|  2|ueficvsxctwefcyui...|\n",
      "|  3|uijgdcuywgutsdzfc...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"gvdchzsvdvduydgivfbsckhfcusdfdsdbcjhyjgfruyegrfbyr\"),\n",
    "       (2, \"ueficvsxctwefcyuiwebfugyefgdsjcvdjgfjdvjsdgfjfghjs\"),\n",
    "       (3, \"uijgdcuywgutsdzfcvytfcvtueyfwytugfvwejfhsgvxuky\")]\n",
    "\n",
    "schema = [\"id\", \"comments\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f802250a-a392-4495-9c9f-d7857967c917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|id |comments                                          |\n",
      "+---+--------------------------------------------------+\n",
      "|1  |gvdchzsvdvduydgivfbsckhfcusdfdsdbcjhyjgfruyegrfbyr|\n",
      "|2  |ueficvsxctwefcyuiwebfugyefgdsjcvdjgfjdvjsdgfjfghjs|\n",
      "|3  |uijgdcuywgutsdzfcvytfcvtueyfwytugfvwejfhsgvxuky   |\n",
      "+---+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eeafd73d-a12a-41cf-aed6-323e142f050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|comments|\n",
      "+---+--------+\n",
      "|  1|   gv...|\n",
      "|  2|   ue...|\n",
      "|  3|   ui...|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "895d20b9-f5a9-49c6-ba10-f2d40cb28564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|comments|\n",
      "+---+--------+\n",
      "|  1|   gv...|\n",
      "|  2|   ue...|\n",
      "+---+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2, truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "233fa41d-335e-42e2-ac41-2af96c87d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------\n",
      " id       | 1                                                  \n",
      " comments | gvdchzsvdvduydgivfbsckhfcusdfdsdbcjhyjgfruyegrfbyr \n",
      "-RECORD 1------------------------------------------------------\n",
      " id       | 2                                                  \n",
      " comments | ueficvsxctwefcyuiwebfugyefgdsjcvdjgfjdvjsdgfjfghjs \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39713d6f-8b07-4677-8d62-fa7dcffe1b8b",
   "metadata": {},
   "source": [
    "# withColumn()\n",
    "\n",
    "PySpark `withColumn()` is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
    "\n",
    "**Syntax:** `DataFrame.withColumn(colName, colExpr)`\n",
    "\n",
    "- `colName`: The name of the new or existing column you want to work with.\n",
    "- `colExpr`: An expression that defines the values for this column. It can be a literal value, an expression using existing columns, or a function applied to one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "905b5e86-4f74-4f51-a94d-19391bd8cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|manish|  3000|\n",
      "|  2|ramesh|  2500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\", 3000), (2, \"ramesh\", 2500)]\n",
    "schema = ['id', 'name', \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7300dde5-c1d5-4de1-b732-2fe39691e10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ceaccdff-e044-4f59-a4e3-697264097d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumn(colName: str, col: pyspark.sql.column.Column) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "    existing column that has the same name.\n",
      "    \n",
      "    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "    a column from some other :class:`DataFrame` will raise an error.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    colName : str\n",
      "        string, name of the new column.\n",
      "    col : :class:`Column`\n",
      "        a :class:`Column` expression for the new column.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        DataFrame with new or replaced column.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This method introduces a projection internally. Therefore, calling it multiple\n",
      "    times, for instance, via loops in order to add multiple columns can generate big\n",
      "    plans which can cause performance issues and even `StackOverflowException`.\n",
      "    To avoid this, use :func:`select` with multiple columns at once.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    >>> df.withColumn('age2', df.age + 2).show()\n",
      "    +---+-----+----+\n",
      "    |age| name|age2|\n",
      "    +---+-----+----+\n",
      "    |  2|Alice|   4|\n",
      "    |  5|  Bob|   7|\n",
      "    +---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4e10713-2979-4f07-996b-e82c6aab4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(colName='salary', col=col('salary').cast('Integer')) # use1: change datatype\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42192e45-6ff9-421c-b3da-fbe17bce48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|manish|  6000|\n",
      "|  2|ramesh|  5000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('salary', col('salary')*2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d64f69d-e70e-418a-9f45-4b67f81f45da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|country|\n",
      "+---+------+------+-------+\n",
      "|  1|manish|  6000|  india|\n",
      "|  2|ramesh|  5000|  india|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = df.withColumn('country', lit('india')) # lit = literal\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1bd6f05-2803-4e32-83ca-7b7553c65de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+-------------+\n",
      "| id|  name|salary|country|copied salary|\n",
      "+---+------+------+-------+-------------+\n",
      "|  1|manish|  6000|  india|         6000|\n",
      "|  2|ramesh|  5000|  india|         5000|\n",
      "+---+------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('copied salary', col('salary'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea4fc5-ae54-41d3-a0c4-d31459addbec",
   "metadata": {},
   "source": [
    "# withColumnRenamed()\n",
    "\n",
    "PySpark `withColumnRenamed()` is a transformation function of DataFrame which is used to change existing column name in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ae5ca4d-1ee6-456f-ab13-16a6db77b7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|manish|  3000|\n",
      "|  2|ramesh|  2500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\", 3000), (2, \"ramesh\", 2500)]\n",
    "schema = ['id', 'name', \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "951753ca-c80e-4c9c-8e59-dca92e59d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|yearly salary|\n",
      "+---+------+-------------+\n",
      "|  1|manish|         3000|\n",
      "|  2|ramesh|         2500|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('salary', 'yearly salary')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1704e-3e05-4d69-b777-e552dcbfc125",
   "metadata": {},
   "source": [
    "# StructType() & StructField()\n",
    "\n",
    "PySpark `StructType` & `StructField` classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8f94246-1913-4169-9d8c-c9ebe7b19fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|manish|  3000|\n",
      "|  2|ramesh|  2500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"manish\", 3000), (2, \"ramesh\", 2500)]\n",
    "schema = ['id', 'name', \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83840a56-1fb3-4b82-aa18-7116115dbe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|manish|  3000|\n",
      "|  2|ramesh|  2500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StructType, IntegerType\n",
    "\n",
    "data = [(1, \"manish\", 3000), (2, \"ramesh\", 2500)]\n",
    "\n",
    "schema = StructType([StructField(name='id', dataType=IntegerType()),\n",
    "                    StructField(name='name', dataType=StringType()),\n",
    "                    StructField(name='salary', dataType=IntegerType())])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d3c1341-fe0b-4f92-b199-b0d00e5de105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb91a4ae-220d-4b2f-8406-21558db553b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+\n",
      "| id|              name|salary|\n",
      "+---+------------------+------+\n",
      "|  1|   {manish, singh}|  3000|\n",
      "|  2|{ramesh, malhotra}|  2500|\n",
      "+---+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StructType, IntegerType\n",
    "\n",
    "data = [(1, (\"manish\", \"singh\"), 3000), (2, (\"ramesh\", \"malhotra\"), 2500)]\n",
    "\n",
    "sturctName = StructType([StructField(name='first name', dataType=StringType()),\n",
    "                        StructField(name='last name', dataType=StringType())])\n",
    "\n",
    "schema = StructType([StructField(name='id', dataType=IntegerType()),\n",
    "                    StructField(name='name', dataType=sturctName),\n",
    "                    StructField(name='salary', dataType=IntegerType())])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b07c9558-8dca-4ba2-bc3a-83a614225fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first name: string (nullable = true)\n",
      " |    |-- last name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22be2f59-8555-41c5-83ff-96c55958684b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: struct<first name:string,last name:string>, salary: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e5e9d-8bf4-4a22-a111-5fb1fdf42260",
   "metadata": {},
   "source": [
    "# ArrayType columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ea94151-2422-4130-9513-7623098beadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|jkl| [3, 4]|\n",
      "|pqr| [5, 6]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('abc', [1,2]), ('jkl', [3,4]), ('pqr', [5,6])]\n",
    "schema = ['id', 'numbers']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce6eb837-f6fb-4b35-9e80-bda17d5badd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04dbb51c-c56c-4c04-b5ea-9ab4c15b97d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|jkl| [3, 4]|\n",
      "|pqr| [5, 6]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('abc', [1,2]), ('jkl', [3,4]), ('pqr', [5,6])]\n",
    "\n",
    "schema = StructType([StructField(name='id', dataType=StringType()),\n",
    "                    StructField(name='numbers', dataType=ArrayType(IntegerType()))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae1560ee-7b53-4883-92f1-45fa15e31d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40c0067b-3bbf-4460-b834-e96d58140cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|numbers|first no.|\n",
      "+---+-------+---------+\n",
      "|abc| [1, 2]|        1|\n",
      "|jkl| [3, 4]|        3|\n",
      "|pqr| [5, 6]|        5|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetching array elements using index\n",
    "df.withColumn('first no.', df.numbers[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2ca6f2f-adba-4c08-86a5-37b2f7862bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,2), (3,4)]\n",
    "schema = ['num1', 'num2']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d2f4a2d-1dbb-4b4e-b39a-140b35a3f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|num1|num2|numbers|\n",
      "+----+----+-------+\n",
      "|   1|   2| [1, 2]|\n",
      "|   3|   4| [3, 4]|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "\n",
    "df.withColumn('numbers', array(col('num1'), col('num2'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e9317-78bb-4a09-80ef-8140fa32d38f",
   "metadata": {},
   "source": [
    "# explode(), split(), array() & array_contains()\n",
    "\n",
    "- Use `explode()` function to create a new row for each element in the given array column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bee51bc9-544c-4d0c-b2da-d1df76d5fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|  name|      skills|\n",
      "+---+------+------------+\n",
      "|  1|ramesh|[Java, .NET]|\n",
      "|  2|rajesh| [Python, R]|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', ['Java', '.NET']), (2, 'rajesh', ['Python', 'R'])]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa41b548-7149-4f4f-80f5-840d52d71303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d038a466-ed62-4a9d-a3a1-38f2616e5ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+------+\n",
      "| id|  name|      skills| skill|\n",
      "+---+------+------------+------+\n",
      "|  1|ramesh|[Java, .NET]|  Java|\n",
      "|  1|ramesh|[Java, .NET]|  .NET|\n",
      "|  2|rajesh| [Python, R]|Python|\n",
      "|  2|rajesh| [Python, R]|     R|\n",
      "+---+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df = df.withColumn('skill', explode(col('skills')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8f865-0a7e-428a-9f03-8a8fb54c2034",
   "metadata": {},
   "source": [
    "- `split()` sql function returns an array type after splitting the string column by delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "069f4ca6-86aa-440e-baa2-c851c1216d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id|  name|     skills|\n",
      "+---+------+-----------+\n",
      "|  1|ramesh|Java, Scala|\n",
      "|  2|rajesh|  Python, R|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'Java, Scala'), (2, 'rajesh', 'Python, R')]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0b967bc-4587-4371-a72d-a9e988d2c3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b225b895-1ab3-407a-952a-5e672742f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------------+\n",
      "| id|  name|     skills|  skills Array|\n",
      "+---+------+-----------+--------------+\n",
      "|  1|ramesh|Java, Scala|[Java,  Scala]|\n",
      "|  2|rajesh|  Python, R|  [Python,  R]|\n",
      "+---+------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "df = df.withColumn('skills Array', split(col('skills'), ','))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11f237-0629-4134-9b87-3c9feb9757cf",
   "metadata": {},
   "source": [
    "- Use `array()` function to create a new array column by merging the data from multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "584a65a1-d3c0-4e69-86e9-a2df14e1baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+---------------+\n",
      "| id|  name|primary skill|secondary skill|\n",
      "+---+------+-------------+---------------+\n",
      "|  1|ramesh|         Java|          Scala|\n",
      "|  2|rajesh|       Python|              R|\n",
      "+---+------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'Java', 'Scala'), (2, 'rajesh', 'Python', 'R')]\n",
    "schema = ['id', 'name', 'primary skill', 'secondary skill']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f573b16-7d41-4e97-b494-08e4ee26217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+---------------+-------------+\n",
      "| id|  name|primary skill|secondary skill| skills array|\n",
      "+---+------+-------------+---------------+-------------+\n",
      "|  1|ramesh|         Java|          Scala|[Java, Scala]|\n",
      "|  2|rajesh|       Python|              R|  [Python, R]|\n",
      "+---+------+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "df = df.withColumn('skills array', array(col('primary skill'), col('secondary skill')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ac2bf-29b4-4b0b-9656-5ec57659cd55",
   "metadata": {},
   "source": [
    "- `array_contains()` sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "03c98ef0-0a59-4801-a10f-9b27f1b003df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|  name|       skills|\n",
      "+---+------+-------------+\n",
      "|  1|ramesh|[Java, Scala]|\n",
      "|  2|rajesh|  [Python, R]|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', ['Java', 'Scala']), (2, 'rajesh', ['Python', 'R'])]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "658d3a36-f1ae-489b-be19-f3c7f8a279bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+---------------+\n",
      "| id|  name|       skills|Has java skill?|\n",
      "+---+------+-------------+---------------+\n",
      "|  1|ramesh|[Java, Scala]|           true|\n",
      "|  2|rajesh|  [Python, R]|          false|\n",
      "+---+------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "df = df.withColumn('Has java skill?', array_contains(col('skills'), 'Java'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9125729-6338-4f11-8d5a-07cb2e716ba0",
   "metadata": {},
   "source": [
    "# MapType columns\n",
    "\n",
    "PySpark `MapType` is used to represent map key-value pair similar to python Dictionarv (Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c55d877-4997-473c-ac51-92a0342c4599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|          properties|\n",
      "+------+--------------------+\n",
      "|ramesh|{eye -> black, ha...|\n",
      "|rajesh|{eye -> blue, hai...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('ramesh', {'hair': 'golden', 'eye': 'black'}), ('rajesh', {'hair': 'black', 'eye': 'blue'})]\n",
    "schema = ['name', 'properties']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3a616be-8bac-4dcd-9564-b62d548d13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91e3b252-a203-4a06-ad1a-1bd8bd742ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+\n",
      "|name  |properties                    |\n",
      "+------+------------------------------+\n",
      "|ramesh|{eye -> black, hair -> golden}|\n",
      "|rajesh|{eye -> blue, hair -> black}  |\n",
      "+------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "data = [('ramesh', {'hair': 'golden', 'eye': 'black'}), ('rajesh', {'hair': 'black', 'eye': 'blue'})]\n",
    "\n",
    "schema = StructType([StructField(name='name', dataType=StringType()),\n",
    "                    StructField(name='properties', dataType=MapType(StringType(), StringType()))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d448068-3181-43ff-8c2f-b331248bcf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b26c76e4-e82a-49d9-9e5b-c2d78acc7dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+------+\n",
      "|name  |properties                    |hair  |\n",
      "+------+------------------------------+------+\n",
      "|ramesh|{eye -> black, hair -> golden}|golden|\n",
      "|rajesh|{eye -> blue, hair -> black}  |black |\n",
      "+------+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('hair', df.properties['hair'])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f70a22b0-e141-4aa8-9ad2-18cd74f0b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+------+-----+\n",
      "|name  |properties                    |hair  |eye  |\n",
      "+------+------------------------------+------+-----+\n",
      "|ramesh|{eye -> black, hair -> golden}|golden|black|\n",
      "|rajesh|{eye -> blue, hair -> black}  |black |blue |\n",
      "+------+------------------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way method\n",
    "\n",
    "df = df.withColumn('eye', df.properties.getItem('eye'))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1c737-8190-41d5-ac01-e955245313fd",
   "metadata": {},
   "source": [
    "# map_keys(), map_values() & explode() functions to work with MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3056aa9-d30b-4f13-ace8-7be93fdea887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+\n",
      "|name  |properties                    |\n",
      "+------+------------------------------+\n",
      "|ramesh|{eye -> black, hair -> golden}|\n",
      "|rajesh|{eye -> blue, hair -> black}  |\n",
      "+------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "data = [('ramesh', {'hair': 'golden', 'eye': 'black'}), ('rajesh', {'hair': 'black', 'eye': 'blue'})]\n",
    "\n",
    "schema = StructType([StructField(name='name', dataType=StringType()),\n",
    "                    StructField(name='properties', dataType=MapType(StringType(), StringType()))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8d3613c7-2bde-412e-b589-5b66963f989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------+----+------+\n",
      "|name  |properties                    |key |value |\n",
      "+------+------------------------------+----+------+\n",
      "|ramesh|{eye -> black, hair -> golden}|eye |black |\n",
      "|ramesh|{eye -> black, hair -> golden}|hair|golden|\n",
      "|rajesh|{eye -> blue, hair -> black}  |eye |blue  |\n",
      "|rajesh|{eye -> blue, hair -> black}  |hair|black |\n",
      "+------+------------------------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = df.select('name', 'properties', explode(df.properties))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78eeebfd-3bc4-4d0d-bc5c-1f14f604ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+------+-----------+\n",
      "|  name|          properties| key| value|       keys|\n",
      "+------+--------------------+----+------+-----------+\n",
      "|ramesh|{eye -> black, ha...| eye| black|[eye, hair]|\n",
      "|ramesh|{eye -> black, ha...|hair|golden|[eye, hair]|\n",
      "|rajesh|{eye -> blue, hai...| eye|  blue|[eye, hair]|\n",
      "|rajesh|{eye -> blue, hai...|hair| black|[eye, hair]|\n",
      "+------+--------------------+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df = df.withColumn('keys', map_keys(df.properties))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa829f49-05c9-48c1-bd8b-4ff0a8341a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+------+-----------+---------------+\n",
      "|  name|          properties| key| value|       keys|        valuess|\n",
      "+------+--------------------+----+------+-----------+---------------+\n",
      "|ramesh|{eye -> black, ha...| eye| black|[eye, hair]|[black, golden]|\n",
      "|ramesh|{eye -> black, ha...|hair|golden|[eye, hair]|[black, golden]|\n",
      "|rajesh|{eye -> blue, hai...| eye|  blue|[eye, hair]|  [blue, black]|\n",
      "|rajesh|{eye -> blue, hai...|hair| black|[eye, hair]|  [blue, black]|\n",
      "+------+--------------------+----+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df = df.withColumn('valuess', map_values(df.properties))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dd28d-8c59-4e3e-a811-bf6e33e5a6d4",
   "metadata": {},
   "source": [
    "# Row() class\n",
    "\n",
    "- `pyspark.sql.Row` which is represented as a record/row in DataFrame, one can create a Row object by using named arguments or create a custom Row like class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ec45c69-964d-4424-b27a-aa0bddbe9c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramesh 2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row('ramesh', 2000)\n",
    "\n",
    "print(row[0] + ' ' + str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b7b029f-9e90-4a89-a84d-e95b053030f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramesh 2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row2 = Row(name='ramesh', salary=2000)\n",
    "\n",
    "print(row2[0] + ' ' + str(row2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "415b6bbe-9f5b-45e8-911c-0a82bb83deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramesh 2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row2 = Row(name='ramesh', salary=2000)\n",
    "\n",
    "print(row2.name + ' ' + str(row2.salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5a3268a4-a2f5-49a0-883d-a9b4eafe469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|rajesh|  3500|\n",
      "|ramesh|  2000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row2 = Row(name='ramesh', salary=2000)\n",
    "row1 = Row(name='rajesh', salary=3500)\n",
    "\n",
    "data = [row1, row2]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55aa412a-ea59-4533-a0e8-c712ca990cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f54eb945-2dfd-443f-975b-bfa7d6c777b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ramesh\n"
     ]
    }
   ],
   "source": [
    "# we can also create a Row like class:\n",
    "\n",
    "Person = Row('name', 'age')\n",
    "\n",
    "persn1 = Person('ramesh', 22)\n",
    "persn2 = Person('rajesh', 25)\n",
    "\n",
    "print(persn1.age, persn1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "191ddc21-d4cc-4f90-b118-2b35a0969aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|ramesh| 22|\n",
      "|rajesh| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([persn1, persn2])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "50044fae-7de2-4642-9574-3865629e33dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|  name|         prop|\n",
      "+------+-------------+\n",
      "|rajesh|{black, blue}|\n",
      "|ramesh|{grey, black}|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can create nested struct type also using Row:\n",
    "\n",
    "data= [Row(name=\"rajesh\", prop=Row(hair=\"black\", eye=\"blue\")),\n",
    "       Row (name=\"ramesh\", prop=Row(hair=\"grey\", eye=\"black\"))]\n",
    "       \n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab5e497e-a8bc-44a4-b2c4-4b26955fee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d88397-3f10-42a8-bd65-dd35108407fe",
   "metadata": {},
   "source": [
    "# Column class\n",
    "\n",
    "- PySpark Column class represents a single Column in a DataFrame.\n",
    "- `pyspark.sql.Column` class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column\n",
    "- One of the simplest ways to create a Column class object is by using PySpark `lit()` SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13a3b8c2-c26f-4ff3-b23b-748273ad1483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "col1 = lit(\"vdcjh\")\n",
    "\n",
    "print(type(col1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "45ff0a90-692d-4dfa-989f-b388dadf0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  name|gender|salary|\n",
      "+------+------+------+\n",
      "|ramesh|  male|  2000|\n",
      "|rajesh|  male|  3000|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('ramesh', 'male', 2000), ('rajesh', 'male', 3000)]\n",
    "schema = ['name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3779f5e3-9ad4-4b68-9cdd-aa0e122278f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0d63098-0da2-4927-8d8b-feaf8d6c69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+\n",
      "|  name|gender|salary|new column|\n",
      "+------+------+------+----------+\n",
      "|ramesh|  male|  2000|     dummy|\n",
      "|rajesh|  male|  3000|     dummy|\n",
      "+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('new column', lit('dummy'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "05470c31-dff0-4ce3-a15f-e20c692bdd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|ramesh|\n",
      "|rajesh|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can access column in multiple ways\n",
    "\n",
    "df.select(df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d59c72b6-0f37-45ad-b284-69d0dbd70fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|ramesh|\n",
      "|rajesh|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7634c70-95ca-4703-8ecf-aec2688434a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|ramesh|\n",
      "|rajesh|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ffded8f-b23e-45ae-b0d2-1945f5cd0406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+--------------+\n",
      "|  name|gender|salary|         props|\n",
      "+------+------+------+--------------+\n",
      "|ramesh|  male|  2000| {black, blue}|\n",
      "|rajesh|  male|  3000|{black, brown}|\n",
      "+------+------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType\n",
    "\n",
    "data = [('ramesh', 'male', 2000, ('black', 'blue')), ('rajesh', 'male', 3000, ('black', 'brown'))]\n",
    "\n",
    "propsType = StructType([StructField(name='hair', dataType=StringType()),\n",
    "                       StructField(name='eye', dataType=StringType())])\n",
    "\n",
    "schema = StructType([StructField(name='name', dataType=StringType()),\n",
    "                    StructField(name='gender', dataType=StringType()),\n",
    "                    StructField(name='salary', dataType=IntegerType()),\n",
    "                    StructField(name='props', dataType=propsType)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e9db1862-81cf-4f16-bfd2-59be28dbe8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- props: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1aaf6391-2c8a-44d0-a033-9f851087dc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|props.eye|\n",
      "+---------+\n",
      "|     blue|\n",
      "|    brown|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.props.eye).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cf0ae439-5ea5-4adb-b5f1-c24afc7e5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  eye|\n",
      "+-----+\n",
      "| blue|\n",
      "|brown|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['props.eye']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a57bf6fb-0f16-4c01-9e07-760b516c7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  eye|\n",
      "+-----+\n",
      "| blue|\n",
      "|brown|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('props.eye')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97940a30-44ce-4fef-a95c-07c4a332526a",
   "metadata": {},
   "source": [
    "# when() & otherwise()\n",
    "\n",
    "It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8304bb74-74bf-43db-9e83-010a5e1ed3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|rajesh|     M|  2000|\n",
      "|  2|ritika|     F|  3000|\n",
      "|  3|rakesh|      |  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'rajesh', 'M', 2000), (2, 'ritika', 'F', 3000), (3, 'rakesh', '', 2000)]\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc0d6e39-43a4-47a9-acd5-da03b6204302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------------------------------------------------------------------+\n",
      "| id|  name|CASE WHEN (gender = M) THEN male WHEN (gender = F) THEN female ELSE unknown END|\n",
      "+---+------+-------------------------------------------------------------------------------+\n",
      "|  1|rajesh|                                                                           male|\n",
      "|  2|ritika|                                                                         female|\n",
      "|  3|rakesh|                                                                        unknown|\n",
      "+---+------+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.select(df.id, df.name, when(condition=df.gender=='M', value='male')\\\n",
    "              .when(condition=df.gender=='F', value='female')\\\n",
    "              .otherwise(value='unknown'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "447af29d-f5c1-41e0-8bfe-bccdb2683645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|rajesh|     M|  2000|\n",
      "|  2|ritika|     F|  3000|\n",
      "|  3|rakesh|      |  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same dataframe\n",
    "data = [(1, 'rajesh', 'M', 2000), (2, 'ritika', 'F', 3000), (3, 'rakesh', '', 2000)]\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2eead44-a27a-4a75-9047-c5aa9e22d3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  name| gender|\n",
      "+---+------+-------+\n",
      "|  1|rajesh|   male|\n",
      "|  2|ritika| female|\n",
      "|  3|rakesh|unknown|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.select(df.id, df.name, when(condition=df.gender=='M', value='male')\\\n",
    "              .when(condition=df.gender=='F', value='female')\\\n",
    "              .otherwise(value='unknown')\\\n",
    "              .alias('gender'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c761348-f239-4156-952e-6ad678f38929",
   "metadata": {},
   "source": [
    "# alias(), asc(), desc(), cast() & like() functions on Columns \n",
    "\n",
    "- `alias()` provides alias to column\n",
    "- `asc()` and `desc()` sort columns in ascending and descending order\n",
    "- `cast()` convert datatype\n",
    "- `like()` similar to SQL LIKE expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "395154e6-2b1a-4120-b726-fe1b611df07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|rajesh|  2000|\n",
      "|  2|ritika|  3000|\n",
      "|  3|rakesh|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'rajesh', 2000), (2, 'ritika', 3000), (3, 'rakesh', 2000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dae390f4-3498-435e-bb16-b7b0b3f5be6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|rajesh|  2000|\n",
      "|  2|ritika|  3000|\n",
      "|  3|rakesh|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name, df.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6cf4685d-2b30-4781-a9e4-95920c1c209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+\n",
      "|employee id|employee name|employee salary|\n",
      "+-----------+-------------+---------------+\n",
      "|          1|       rajesh|           2000|\n",
      "|          2|       ritika|           3000|\n",
      "|          3|       rakesh|           2000|\n",
      "+-----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias()\n",
    "df.select(df.id.alias('employee id'), df.name.alias('employee name'), df.salary.alias('employee salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc3299f5-d1aa-4f91-85bb-378f7de8da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|rajesh|  2000|\n",
      "|  3|rakesh|  2000|\n",
      "|  2|ritika|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc()\n",
    "df.sort(df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f4efcad1-d95f-4a35-a1a8-ba87a2ed532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  2|ritika|  3000|\n",
      "|  3|rakesh|  2000|\n",
      "|  1|rajesh|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# desc()\n",
    "df.sort(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fda42785-f69d-47d4-aaa2-ac8cb3dfee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "98f33035-c124-4b5e-bd15-c32e40e03ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cast()\n",
    "df = df.select(df.id, df.name, df.salary.cast('int'))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a80b716c-e70c-44b9-b1a6-65ea8e521c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|rajesh|  2000|\n",
      "|  2|ritika|  3000|\n",
      "|  3|rakesh|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# like()\n",
    "df.filter(df.name.like('r%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3153df46-caa0-411f-8e81-fa8417992d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|rajesh|  2000|\n",
      "|  3|rakesh|  2000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# like()\n",
    "df.filter(df.name.like('%esh')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabc3de-3659-4070-b87a-08a864906a63",
   "metadata": {},
   "source": [
    "# filter() & where()\n",
    "\n",
    "- PySpark `filter()` function is used to filter the rows from DataFrame based on the given condition or SQL expression.\n",
    "- You can also use where() clause instead of the filter) if you are coming from an SQL background, both these functions operate exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "96edd6cc-f054-4cf2-920e-8edbc8b14ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "|  3|surbhi|female|  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'male', 2000), (2, 'suresh', 'male', 3000), (3, 'surbhi', 'female', 4000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'income']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "393e0f7c-c538-4d92-9aaf-ed67eda379b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.gender == 'male').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "510503ce-93cf-4450-8d34-da5d9fdbf6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"gender == 'male'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6cef98b-57cc-4d54-b303-ee4eef727a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"gender == 'male'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f666ee58-45fa-491f-aa68-edba50bdb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  2|suresh|  male|  3000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((df.gender == 'male') & (df.income > 2500)).show() # multiple conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7335c55-054d-4239-a505-f107ee7ca53e",
   "metadata": {},
   "source": [
    "# distinct() & dropDuplicates()\n",
    "\n",
    "- PySpark `distinct()` function is used to remove the duplicate rows (all columns)\n",
    "- `dropDuplicates()` is used to drop rows based on selected (one or multiple)\n",
    "- So basically, using these functions we can get distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08aaded2-7971-4f98-8061-7a2876687fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "|  2|suresh|  male|  3000|\n",
      "|  3|surbhi|female|  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'male', 2000), (2, 'suresh', 'male', 3000), (2, 'suresh', 'male', 3000), (3, 'surbhi', 'female', 4000)]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'income']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b409400c-d7e1-4f33-86dd-4bd61216c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "|  3|surbhi|female|  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8044ebad-6073-4e17-a664-35423431b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  2000|\n",
      "|  2|suresh|  male|  3000|\n",
      "|  3|surbhi|female|  4000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e58d4d07-1d4b-437b-94a0-d26e7e1a2441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|income|\n",
      "+---+------+------+------+\n",
      "|  3|surbhi|female|  4000|\n",
      "|  1|ramesh|  male|  2000|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['gender']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb412ad8-02f0-4d4a-b5a6-ef6eefd72208",
   "metadata": {},
   "source": [
    "# orderBy() & sort()\n",
    "\n",
    "- You can use either `sort()` or `orderBy()` function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns.\n",
    "- By default, sorting will happen in ascending order. We can explicitly mention ascending or descending using asc(), desc() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24503cc4-478c-4645-be88-fd8500da32a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'male', 2000, 'HR'), \n",
    "        (2, 'suresh', 'male', 3000, 'IT'), \n",
    "        (3, 'sukesh', 'male', 3500, 'sales'), \n",
    "        (4, 'surbhi', 'female', 4000, 'HR')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aed21460-73bf-431f-9d6a-333e771baa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('department').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83836ced-8a3a-495c-8550-d24a0af27558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "561fe5e2-f42b-4fe5-9d95-f0c028a0dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department, df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae1ba81c-aa34-4cbe-9f3c-ffbc8aac3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department, df.id.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b87b5f34-fe7f-4532-9b29-f14ea7d159e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.department).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94784650-cf10-4975-8a68-ba79c72ccfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.department, df.id.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3997991-496e-4ee2-9839-f0a16f388011",
   "metadata": {},
   "source": [
    "# union() & unionAll()\n",
    "\n",
    "- `union()` and `unionAll()` transformations are used to merge two or more DataFrame's of the same schema or structure.\n",
    "- `union()` & `unionAll()` method merges two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data.\n",
    "- To remove duplicates use `distinct()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f338af2-6dc3-495c-a016-5f49d81d51d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1, 'ramesh', 'male', 2000, 'HR'), \n",
    "        (2, 'suresh', 'male', 3000, 'IT')]\n",
    "\n",
    "schema1 = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "058ad76f-a5c2-4526-99c2-c55a28cb2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [ (3, 'sukesh', 'male', 3500, 'sales'), \n",
    "        (4, 'surbhi', 'female', 4000, 'HR')]\n",
    "\n",
    "schema2 = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c339c96b-d5ec-4809-80e0-380a71d5388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf = df1.union(df2)\n",
    "\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcb6fc91-e7d1-442d-bcc1-d161531c41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1, 'ramesh', 'male', 2000, 'HR'), \n",
    "        (2, 'suresh', 'male', 3000, 'IT'),\n",
    "        (2, 'suresh', 'male', 3000, 'IT')]\n",
    "data2 = [ (3, 'sukesh', 'male', 3500, 'sales'), \n",
    "        (4, 'surbhi', 'female', 4000, 'HR')]\n",
    "\n",
    "schema1 = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "schema2 = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52f4c776-bfdc-40be-a391-e7afd27f0d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf = df1.union(df2)\n",
    "\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed6d8352-3844-4c2e-9945-e0225708e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf = df1.unionAll(df2)\n",
    "\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf2c5b4b-c5b8-48ab-ba6e-b1fd2ff83438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0ef4e-0faf-495a-bfb2-c5a2ce88750e",
   "metadata": {},
   "source": [
    "# groupBy() \n",
    "\n",
    "- Similar to SQL GROUP BY clause, PySpark `groupBy()` function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "acdc2fa8-bda5-4ab0-88e9-763a6c1d113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|        HR|\n",
      "|  2|suresh|  male|  3000|        IT|\n",
      "|  3|sukesh|  male|  3500|     sales|\n",
      "|  5|mukesh|  male|  3300|        IT|\n",
      "|  4|surbhi|female|  4000|        HR|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 'male', 2000, 'HR'), \n",
    "        (2, 'suresh', 'male', 3000, 'IT'), \n",
    "        (3, 'sukesh', 'male', 3500, 'sales'), \n",
    "        (5, 'mukesh', 'male', 3300, 'IT'), \n",
    "        (4, 'surbhi', 'female', 4000, 'HR')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a323a436-5c84-434f-a6f8-943442abd1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|        HR|    2|\n",
      "|        IT|    2|\n",
      "|     sales|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('department').count()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "950bb5f7-6567-4588-a2e6-933aaab21139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|        HR|       2000|\n",
      "|        IT|       3000|\n",
      "|     sales|       3500|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(df.department).min('salary')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "680437d4-a96d-4685-ae23-0d25fe833f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|department|gender|count|\n",
      "+----------+------+-----+\n",
      "|        HR|  male|    1|\n",
      "|        IT|  male|    2|\n",
      "|     sales|  male|    1|\n",
      "|        HR|female|    1|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(df.department, df.gender).count() # multiple columns\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d1a73-ef16-418a-b382-760572bf4112",
   "metadata": {},
   "source": [
    "# groupBy().agg()\n",
    "\n",
    "- PySpark Groupby `agg()` is used to calculate more than one aggregate (multiple aggregates) at a time on grouped DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57ed046e-63ef-4457-99e3-4daa053b38c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|count of employee|\n",
      "+----------+-----------------+\n",
      "|        HR|                2|\n",
      "|        IT|                2|\n",
      "|     sales|                1|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, min, max\n",
    "\n",
    "df.groupBy('department').agg(count('*').alias('count of employee')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2e2b2f7-20f2-42ab-bc9b-c49a5985f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+-----------+\n",
      "|department|count of employee|Min. salary|Max. salary|\n",
      "+----------+-----------------+-----------+-----------+\n",
      "|        HR|                2|       2000|       4000|\n",
      "|        IT|                2|       3000|       3300|\n",
      "|     sales|                1|       3500|       3500|\n",
      "+----------+-----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(count('*').alias('count of employee'),\n",
    "                            min('salary').alias('Min. salary'),\n",
    "                            max('salary').alias('Max. salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb480446-e7f5-47d8-80c2-26914ac774d9",
   "metadata": {},
   "source": [
    "# unionByName()\n",
    "\n",
    "- `unionByName()` lets you to merge/union two DataFrames with a different number of columns (different schema) by passing `allowMissingColumns` with the value true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2563412-69ee-4372-a0a4-13715dca4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1, 'ramesh', 23)]\n",
    "schema1 = ['id', 'name', 'age']\n",
    "\n",
    "data2 = [(1, 'ramesh', 3500)]\n",
    "schema2 = ['id', 'name', 'salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37cb8e92-c729-4c60-a24a-6166ed0cbc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|ramesh| 23|\n",
      "+---+------+---+\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|ramesh|  3500|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6183365e-dad9-47eb-8f9f-66f9ed73aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|  name| age|\n",
      "+---+------+----+\n",
      "|  1|ramesh|  23|\n",
      "|  1|ramesh|3500|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72516612-cc94-4b16-8d94-05dd35160d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"age\" among (id, name, salary).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munionByName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow() \u001b[38;5;66;03m# ERROR\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:4043\u001b[0m, in \u001b[0;36mDataFrame.unionByName\u001b[1;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munionByName\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, allowMissingColumns: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3966\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3968\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4041\u001b[0m \u001b[38;5;124;03m    +----+----+----+----+----+----+\u001b[39;00m\n\u001b[0;32m   4042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munionByName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowMissingColumns\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Programs\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"age\" among (id, name, salary)."
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2).show() # ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db88b6ad-2684-448b-a3c6-c2ebdc118455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+\n",
      "| id|  name| age|salary|\n",
      "+---+------+----+------+\n",
      "|  1|ramesh|  23|  NULL|\n",
      "|  1|ramesh|NULL|  3500|\n",
      "+---+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2, allowMissingColumns=True).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399d080-a7c1-47ff-8fb2-176ae89c8904",
   "metadata": {},
   "source": [
    "# select()\n",
    "\n",
    "- `select()` function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "204ef6ac-2f1f-48c9-95f0-59518551d92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  3500|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh','male', 3500)]\n",
    "schema = ['id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b651247-dc2d-498f-a0c9-b3a0f75c0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d50fa739-3c9c-4bd0-9130-853e5175ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04dbb7ae-7bc8-4b41-9fad-b105aea76fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['id'], df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03b6bc79-b6a9-4c06-92a8-d0ce2213250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col('id'), col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7be93ee-1e2d-4e18-ba5c-f8797b5cfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|ramesh|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['id', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "603a7d61-0b8e-4667-8dd0-456cbaada36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  3500|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70375674-ea29-42d9-acf1-b08d018fc4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|  name|gender|salary|\n",
      "+---+------+------+------+\n",
      "|  1|ramesh|  male|  3500|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([col for col in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cbe5-2ae4-462f-9ba7-69103dc72163",
   "metadata": {},
   "source": [
    "# join()\n",
    "\n",
    "- `join()` is like SQL JOIN. We can combine columns from different DataFrames based on condition. It supports all basic join types such as INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "550eaf44-c920-4e61-b883-fcc065acd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1, 'ramesh', 'male', 2000, 2), \n",
    "        (2, 'suresh', 'male', 3000, 1), \n",
    "        (3, 'sukesh', 'male', 3500, 3), \n",
    "        (5, 'mukesh', 'male', 3300, 1), \n",
    "        (4, 'surbhi', 'female', 4000, 4)]\n",
    "\n",
    "schema1 = ['id', 'name', 'gender', 'salary', 'department']\n",
    "\n",
    "data2 = [(1, 'IT'), (2, 'HR'), (3, 'sales')]\n",
    "schema2 = ['dept id', 'dept name']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76535f9c-170d-42eb-9f3e-41f9fc016629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  1|ramesh|  male|  2000|         2|\n",
      "|  2|suresh|  male|  3000|         1|\n",
      "|  3|sukesh|  male|  3500|         3|\n",
      "|  5|mukesh|  male|  3300|         1|\n",
      "|  4|surbhi|female|  4000|         4|\n",
      "+---+------+------+------+----------+\n",
      "\n",
      "+-------+---------+\n",
      "|dept id|dept name|\n",
      "+-------+---------+\n",
      "|      1|       IT|\n",
      "|      2|       HR|\n",
      "|      3|    sales|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7c6c840-b6ef-40c6-a2da-5c9f32bbafc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+-------+---------+\n",
      "| id|  name|gender|salary|department|dept id|dept name|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "|  2|suresh|  male|  3000|         1|      1|       IT|\n",
      "|  5|mukesh|  male|  3300|         1|      1|       IT|\n",
      "|  1|ramesh|  male|  2000|         2|      2|       HR|\n",
      "|  3|sukesh|  male|  3500|         3|      3|    sales|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.department == df2['dept id'], 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1f4f6eb0-e718-40b4-8be1-a55effedc26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+-------+---------+\n",
      "| id|  name|gender|salary|department|dept id|dept name|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "|  1|ramesh|  male|  2000|         2|      2|       HR|\n",
      "|  2|suresh|  male|  3000|         1|      1|       IT|\n",
      "|  3|sukesh|  male|  3500|         3|      3|    sales|\n",
      "|  5|mukesh|  male|  3300|         1|      1|       IT|\n",
      "|  4|surbhi|female|  4000|         4|   NULL|     NULL|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.department == df2['dept id'], 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e816d14-4a55-4650-b78d-5714994e1837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+-------+---------+\n",
      "| id|  name|gender|salary|department|dept id|dept name|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "|  2|suresh|  male|  3000|         1|      1|       IT|\n",
      "|  5|mukesh|  male|  3300|         1|      1|       IT|\n",
      "|  1|ramesh|  male|  2000|         2|      2|       HR|\n",
      "|  3|sukesh|  male|  3500|         3|      3|    sales|\n",
      "|  4|surbhi|female|  4000|         4|   NULL|     NULL|\n",
      "+---+------+------+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.department == df2['dept id'], 'full').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1f547-0c0a-42b9-892f-834b62affa1c",
   "metadata": {},
   "source": [
    "- leftsemi join similar to inner join but get columns only from left dataframe for matching rows.\n",
    "- leftanti opposite to leftsemi, it gets not matching rows from left dataframe.\n",
    "- Self join, joins data with same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c43aa192-18f3-420b-90ba-9d84b67e2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  2|suresh|  male|  3000|         1|\n",
      "|  5|mukesh|  male|  3300|         1|\n",
      "|  1|ramesh|  male|  2000|         2|\n",
      "|  3|sukesh|  male|  3500|         3|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.department == df2['dept id'], 'leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ac949e4-aa15-4e63-96f6-c07e5592b43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "| id|  name|gender|salary|department|\n",
      "+---+------+------+------+----------+\n",
      "|  4|surbhi|female|  4000|         4|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.department == df2['dept id'], 'leftanti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7df488d8-c7f7-4a9c-a488-4d3aff13189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  name|manager_id|\n",
      "+---+------+----------+\n",
      "|  1|ramesh|         0|\n",
      "|  2|suresh|         1|\n",
      "|  3|surbhi|         2|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'ramesh', 0), (2, 'suresh', 1), (3, 'surbhi', 2)]\n",
    "schema = ['id', 'name', 'manager_id']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d318119-67f6-4774-a2ec-6eec67547603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---+------+----------+\n",
      "| id|  name|manager_id| id|  name|manager_id|\n",
      "+---+------+----------+---+------+----------+\n",
      "|  1|ramesh|         0|  2|suresh|         1|\n",
      "|  2|suresh|         1|  3|surbhi|         2|\n",
      "+---+------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.alias('empData').join(df.alias('mngrData'), col('empData.id') == col('mngrData.manager_id'), 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7dd604f3-1b62-42bb-b5c3-78d2b1a1c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---+------+----------+\n",
      "| id|  name|manager_id| id|  name|manager_id|\n",
      "+---+------+----------+---+------+----------+\n",
      "|  1|ramesh|         0|  2|suresh|         1|\n",
      "|  2|suresh|         1|  3|surbhi|         2|\n",
      "+---+------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('empData').join(df.alias('mngrData'), col('mngrData.manager_id') == col('empData.id'), 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "310513f0-2e7f-4ac2-9ae5-7941b1e8a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n",
      "| id|  name|manager_id|  id|  name|manager_id|\n",
      "+---+------+----------+----+------+----------+\n",
      "|  1|ramesh|         0|   2|suresh|         1|\n",
      "|  2|suresh|         1|   3|surbhi|         2|\n",
      "|  3|surbhi|         2|NULL|  NULL|      NULL|\n",
      "+---+------+----------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('empData').join(df.alias('mngrData'), col('empData.id') == col('mngrData.manager_id'), 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "052bfb48-51a1-4f00-9350-33fbd5ffe1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|emp name|manager name|\n",
      "+--------+------------+\n",
      "|  ramesh|      suresh|\n",
      "|  suresh|      surbhi|\n",
      "|  surbhi|        NULL|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('empData').join(df.alias('mngrData'), col('empData.id') == col('mngrData.manager_id'), 'left')\\\n",
    ".select(col('empData.name').alias('emp name'), col('mngrData.name').alias('manager name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4551df-d77a-4373-9d7c-61d3d8c14328",
   "metadata": {},
   "source": [
    "# pivot()\n",
    "\n",
    "- It's used to rotate data in one column into multiple columns.\n",
    "- It is an aggregation where one of the grouping column values will be converted in individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a1631b0-f089-41f2-9d68-0281a228c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+\n",
      "| id|    name|gender|dep|\n",
      "+---+--------+------+---+\n",
      "|  1|  maheer|  male| IT|\n",
      "|  2|    wafa|  male| IT|\n",
      "|  3|     asi|female| HR|\n",
      "|  4|    annu|female| IT|\n",
      "|  5|  shakti|female| IT|\n",
      "|  6| pradeep|  male| HR|\n",
      "|  7|sarfaraj|  male| HR|\n",
      "|  8|  ayesha|female| IT|\n",
      "+---+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'male', 'IT'),\n",
    "        (2, 'wafa', 'male','IT'),\n",
    "        (3, 'asi', 'female', 'HR'),\n",
    "        (4, 'annu', 'female', 'IT'),\n",
    "        (5, 'shakti', 'female', 'IT'),\n",
    "        (6, 'pradeep', 'male' , 'HR'), \n",
    "        (7, 'sarfaraj', 'male', 'HR'),\n",
    "        (8, 'ayesha', 'female' , 'IT')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61a181da-623f-4751-a5af-8fe55c82befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|dep|gender|count|\n",
      "+---+------+-----+\n",
      "| IT|  male|    2|\n",
      "| HR|female|    1|\n",
      "| IT|female|    3|\n",
      "| HR|  male|    2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dep', 'gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "63304238-09d0-4ffd-8c5d-ff2393412846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "|dep|female|male|\n",
      "+---+------+----+\n",
      "| HR|     1|   2|\n",
      "| IT|     3|   2|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dep').pivot('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53e3a81a-00e9-434a-bca5-f6a97b8e73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|dep|male|\n",
      "+---+----+\n",
      "| HR|   2|\n",
      "| IT|   2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dep').pivot('gender', ['male']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5fe264af-2a1f-4f20-a776-48ddd821e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|dep|male|female|\n",
      "+---+----+------+\n",
      "| HR|   2|     1|\n",
      "| IT|   2|     3|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dep').pivot('gender', ['male', 'female']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ac83b-ffec-4250-a074-65058969cd1c",
   "metadata": {},
   "source": [
    "# unpivot\n",
    "\n",
    "- Unpivot is rotating columns into rows. PySpark SQL doesn't have unpivot function hence will use the `stack()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c94c2d1a-79ac-4ad1-9c25-de2fbc111b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|    dep|male|female|\n",
      "+-------+----+------+\n",
      "|     IT|   8|     5|\n",
      "|Payroll|   3|     2|\n",
      "|     HR|   2|     4|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('IT', 8, 5),\n",
    "        ('Payroll', 3, 2), \n",
    "        ('HR', 2, 4)]\n",
    "\n",
    "schema = ['dep', 'male', 'female']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6e6daf85-eaab-4f7d-935c-dc08706f3154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|    dep|gender|count|\n",
      "+-------+------+-----+\n",
      "|     IT|     M|    8|\n",
      "|     IT|     F|    5|\n",
      "|Payroll|     M|    3|\n",
      "|Payroll|     F|    2|\n",
      "|     HR|     M|    2|\n",
      "|     HR|     F|    4|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "unpivotDf = df.select('dep', expr(\"stack(2, 'M', male, 'F', female) as (gender, count)\"))\n",
    "unpivotDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba302f8-1ac3-41b9-8997-f26cc27e4859",
   "metadata": {},
   "source": [
    "# fill() & fillna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5cb68011-8c05-4e8d-9b1c-3fd88802208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----+\n",
      "| id|  name|gender|salary| dep|\n",
      "+---+------+------+------+----+\n",
      "|  1|Maheer|  male|  1000|NULL|\n",
      "|  2|   Asi|Female|  2000|  IT|\n",
      "|  3|  abcd|  NULL|  1000|  HR|\n",
      "+---+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Maheer', 'male', 1000, None),\n",
    "        (2, 'Asi', 'Female' , 2000, 'IT'),\n",
    "        (3, 'abcd' , None, 1000, 'HR' )]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'salary', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48796f72-12ab-4234-9907-c3f5263e56c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+-------+\n",
      "| id|  name| gender|salary|    dep|\n",
      "+---+------+-------+------+-------+\n",
      "|  1|Maheer|   male|  1000|unknown|\n",
      "|  2|   Asi| Female|  2000|     IT|\n",
      "|  3|  abcd|unknown|  1000|     HR|\n",
      "+---+------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('unknown').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bda086f1-c97e-4849-a473-530271d18774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+----+\n",
      "| id|  name| gender|salary| dep|\n",
      "+---+------+-------+------+----+\n",
      "|  1|Maheer|   male|  1000|NULL|\n",
      "|  2|   Asi| Female|  2000|  IT|\n",
      "|  3|  abcd|unknown|  1000|  HR|\n",
      "+---+------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('unknown', ['gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b390ed9-fdaa-47a1-aff2-7299bac17089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+-------+\n",
      "| id|  name| gender|salary|    dep|\n",
      "+---+------+-------+------+-------+\n",
      "|  1|Maheer|   male|  1000|unknown|\n",
      "|  2|   Asi| Female|  2000|     IT|\n",
      "|  3|  abcd|unknown|  1000|     HR|\n",
      "+---+------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna('unknown', ['gender', 'dep']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aec3300b-c91b-44ef-9413-faef07374200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+----+\n",
      "| id|  name| gender|salary| dep|\n",
      "+---+------+-------+------+----+\n",
      "|  1|Maheer|   male|  1000|NULL|\n",
      "|  2|   Asi| Female|  2000|  IT|\n",
      "|  3|  abcd|unknown|  1000|  HR|\n",
      "+---+------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('unknown', ['gender']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a630e-16de-4de6-b00a-2bdaf2a6fbbf",
   "metadata": {},
   "source": [
    "# sample()\n",
    "\n",
    "- To get the random sampling subset from the large dataset\n",
    "- Use fraction to indicate what percentage of data to return and seed value to make sure every time to get same random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0fedca12-3f2b-4a83-8ab1-701ebf8734ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(start=1, end=101) # 101 excluded\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c8736108-d57a-4302-89dd-d28572156ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "| 22|\n",
      "| 23|\n",
      "| 28|\n",
      "| 34|\n",
      "| 37|\n",
      "| 42|\n",
      "| 43|\n",
      "| 51|\n",
      "| 56|\n",
      "| 64|\n",
      "| 82|\n",
      "| 88|\n",
      "| 96|\n",
      "| 99|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.sample(fraction=0.1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1a216652-d52e-484e-9913-6b6b0930a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "| 17|\n",
      "| 21|\n",
      "| 25|\n",
      "| 30|\n",
      "| 40|\n",
      "| 61|\n",
      "| 67|\n",
      "| 83|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.sample(fraction=0.1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d01a32cf-2a1c-4cf9-b7a3-8eedb77b8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 36|\n",
      "| 39|\n",
      "| 42|\n",
      "| 46|\n",
      "| 72|\n",
      "| 85|\n",
      "| 88|\n",
      "|100|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.sample(fraction=0.1, seed=123)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d26ad43d-2ab4-4dd0-9623-53fe51b51224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 36|\n",
      "| 39|\n",
      "| 42|\n",
      "| 46|\n",
      "| 72|\n",
      "| 85|\n",
      "| 88|\n",
      "|100|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.sample(fraction=0.1, seed=123)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af7a1c-3d99-4ab4-ade7-20cc81e8edd2",
   "metadata": {},
   "source": [
    "#  collect()\n",
    "\n",
    "- `collect()` retrieves all elements in a DataFrame as an Array of Row type to the driver node.\n",
    "- `collect()` is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver. Once the data is in an array, you can use python for loop to\n",
    "process it further.\n",
    "- `collect()` use it with small DataFrames. With big DataFrames it may result in out of memory error as its return entier data to single node (driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77a59395-1d75-45f1-9f7b-fc174b1ec911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|maheer|  2000|\n",
      "|  2|  wafa|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000), (2, 'wafa', 3000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3a76c064-0786-4755-a37a-be526d421176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select('id')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5e6c20f-11af-4537-b63f-efe7d994ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='maheer', salary=2000), Row(id=2, name='wafa', salary=3000)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRows = df.collect()\n",
    "listRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45848935-23c7-4510-8790-099e3a9f2228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=1, name='maheer', salary=2000)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "666c7566-e79e-485d-9fca-b95e2cf2180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maheer'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRows[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ca68695d-8800-4ee7-b128-afb27d173d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maheer'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRows[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051feff3-885e-4ba3-b75a-50563e63e264",
   "metadata": {},
   "source": [
    "# DataFrame.transform()\n",
    "\n",
    "- It's is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a10d391-1b00-4099-bbd1-32260c5bf8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|maheer|  2000|\n",
      "|  2|  wafa|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000), (2, 'wafa', 3000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7077953-f818-48b2-b78d-9984316d6f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|MAHEER|  2000|\n",
      "|  2|  WAFA|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "def convertNameToUpper(df):\n",
    "    return df.withColumn('name', upper(df.name))\n",
    "\n",
    "df = df.transform(convertNameToUpper)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ef7260af-827d-4ec9-9432-34ea524b2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|MAHEER|  4000|\n",
      "|  2|  WAFA|  6000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def doubleSalary(df):\n",
    "    return df.withColumn('salary', df.salary*2)\n",
    "\n",
    "df = df.transform(doubleSalary)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0e06cf00-8e87-419d-885e-e602e217f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|MAHEER|  8000|\n",
      "|  2|  WAFA| 12000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.transform(convertNameToUpper).transform(doubleSalary)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7ad34-8bce-4ae2-b3b6-ce34e3c2791b",
   "metadata": {},
   "source": [
    "# pyspark.sql.functions.transform()\n",
    "\n",
    "- It's is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of `ArrayType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1684fa1-5cce-4844-aacd-220d37f25798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|  name|         skills|\n",
      "+---+------+---------------+\n",
      "|  1|maheer|[azure, dotnet]|\n",
      "|  2|  wafa|    [aws, java]|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer' , ['azure', 'dotnet']), (2, 'wafa', ['aws', 'java'])]\n",
    "schema = ['id', 'name', 'skills']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cf074a67-4948-46e6-a5f6-52aa6aa7a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9fb922a8-a369-4d8f-abd9-f76c38a76585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|  name|         skills|\n",
      "+---+------+---------------+\n",
      "|  1|maheer|[AZURE, DOTNET]|\n",
      "|  2|  wafa|    [AWS, JAVA]|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import transform, upper\n",
    "\n",
    "df.select('id', 'name', transform('skills', lambda x: upper(x)).alias('skills')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "28bbe525-c4fc-41a6-8a23-b5ceaebe01bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|  name|         skills|\n",
      "+---+------+---------------+\n",
      "|  1|maheer|[azure, dotnet]|\n",
      "|  2|  wafa|    [aws, java]|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ed6a10f8-9c4d-454c-9b54-778dab57fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------------------------------------------------------------------------+\n",
      "| id|  name|transform(skills, lambdafunction(upper(namedlambdavariable()), namedlambdavariable()))|\n",
      "+---+------+--------------------------------------------------------------------------------------+\n",
      "|  1|maheer|                                                                       [AZURE, DOTNET]|\n",
      "|  2|  wafa|                                                                           [AWS, JAVA]|\n",
      "+---+------+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convertArrayToUpper(x):\n",
    "    return upper(x)\n",
    "    \n",
    "df.select('id', 'name', transform('skills', convertArrayToUpper)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1356e2a-091d-47d3-a67b-0a09525997c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|  name|         skills|\n",
      "+---+------+---------------+\n",
      "|  1|maheer|[AZURE, DOTNET]|\n",
      "|  2|  wafa|    [AWS, JAVA]|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convertArrayToUpper(x):\n",
    "    return upper(x)\n",
    "    \n",
    "df.select('id', 'name', transform('skills', convertArrayToUpper).alias('skills')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29be5-7e78-4251-863f-5637dddb4688",
   "metadata": {},
   "source": [
    "# createOrReplaceTempView()\n",
    "\n",
    "- Advantage of Spark, you can work with SQL along with DataFrames. That means, if you are comfortable with SQL, you can create temporary view on Dataframe by using createOrReplace TempView() and use SQL to select and manipulate data.\n",
    "- Temp Views are session scoped and cannot be shared between the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ead4ee65-5b29-4529-a925-7ef7586e5dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|maheer|  2000|\n",
      "|  2|  wafa|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000), (2, 'wafa', 3000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fddeb2d3-fc1e-4ffa-8368-943dedb1be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|maheer|\n",
      "|  2|  wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select('id', 'name')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f95b119-fab1-46be-9d46-42bc257059d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|maheer|\n",
      "|  2|  wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('employees')\n",
    "\n",
    "df1 = spark.sql(\"select id, name from employees\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93b179-6f82-4b66-829b-b980fa423a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql # for databricks\n",
    "select id, upper(name) from employees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d02637-b5da-48b6-9c90-05e7b4581b5a",
   "metadata": {},
   "source": [
    "# createOrReplaceGlobalTempView()\n",
    "\n",
    "- It's used to create temp views or tables globally, when can be accessed across the sessions with in Spark Application.\n",
    "- To query these tables, we need append `global_temp.<tablename>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f606c817-dd6c-472c-8953-24dd810f87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|maheer|  2000|\n",
      "|  2|  wafa|  3000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000), (2, 'wafa', 3000)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6df6ec53-1f77-427b-b5f1-9af6a6d635f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|maheer|\n",
      "|  2|  wafa|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('globalEmp')\n",
    "\n",
    "df1 = spark.sql(\"select id, name from global_temp.globalEmp\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2d107bfe-1f7b-4d15-a60d-660e107a677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aabcd203-ca11-4830-b298-5ab83b2927ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "71d31c73-2878-42a3-b91e-246090d461c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d127374b-d080-41ce-92b7-7f06e025f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='globalEmp', catalog=None, namespace=['global_temp'], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('global_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6259c4af-fc59-4910-a31b-9671331d97a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView('globalEmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3dde70-c735-47e0-b513-7936b2030662",
   "metadata": {},
   "source": [
    "# user defined function\n",
    "\n",
    "- These are similar to functions in SQL. We define some logic in functions and store them in Database and use them in queries.\n",
    "- Similar to that we can write our own custom logic in python function and register it with PySpark using `udf()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2c940e4f-c811-48f5-90d5-7bc7824babee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n",
      "| id|  name|salary|bonus|\n",
      "+---+------+------+-----+\n",
      "|  1|maheer|  2000|  500|\n",
      "|  2|  wafa|  4000| 1000|\n",
      "+---+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000, 500), (2, 'wafa', 4000, 1000)]\n",
    "schema = ['id', 'name', 'salary', 'bonus']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dfacade6-a73d-4caa-b72c-d732baef6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalPay(sal, bon):\n",
    "    return sal+bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b197f94e-4822-41c7-bd1e-adc335cc545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "totalPayment = udf(lambda sal, bon: totalPay(sal, bon), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c6714a7-da6e-416c-85dc-27a421441e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+------------+\n",
      "| id|  name|salary|bonus|total salary|\n",
      "+---+------+------+-----+------------+\n",
      "|  1|maheer|  2000|  500|        2500|\n",
      "|  2|  wafa|  4000| 1000|        5000|\n",
      "+---+------+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('total salary', totalPayment(df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "216a347b-6784-4736-9548-dd0e659bb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to register UDF\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def totalPay(sal, bon):\n",
    "    return sal+bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d18fb979-cb3d-4ddb-80c3-9ed31810e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----------------------+\n",
      "| id|  name|salary|bonus|totalPay(salary, bonus)|\n",
      "+---+------+------+-----+-----------------------+\n",
      "|  1|maheer|  2000|  500|                   2500|\n",
      "|  2|  wafa|  4000| 1000|                   5000|\n",
      "+---+------+------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*', totalPay(df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "172ed291-8dfe-4c64-bbf8-908224974f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+----------+\n",
      "| id|  name|salary|bonus|using @udf|\n",
      "+---+------+------+-----+----------+\n",
      "|  1|maheer|  2000|  500|      2500|\n",
      "|  2|  wafa|  4000| 1000|      5000|\n",
      "+---+------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*', totalPay(df.salary, df.bonus).alias('using @udf')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1b064cc9-29bd-4aac-a282-0ad26e0292b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.totalPay(sal, bon)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register UDF so that it can be used in SQL as well\n",
    "\n",
    "def totalPay(sal, bon):\n",
    "    return sal+bon\n",
    "\n",
    "spark.udf.register(name='totalPaySQL', f=totalPay, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "28295dc7-3f3d-44f8-8258-0186f6352b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|totSal|\n",
      "+---+------+------+\n",
      "|  1|maheer|  2500|\n",
      "|  2|  wafa|  5000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('employees')\n",
    "\n",
    "df1 = spark.sql(\"select id, name, totalPaySQL(salary, bonus) as totSal from employees\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c6b59-988b-4a1d-80e8-3dc945d1c42d",
   "metadata": {},
   "source": [
    "# Convert RDD(Resilient distributed dataset) to Dataframe\n",
    "\n",
    "- Its collection of objects similar to list in Python. Its Immutable and In memory processing.\n",
    "- By using `parallelize()` function of SparkContext you can create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a7bbb365-4cc9-413c-8710-5344e283437b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, 'maheer', 2000, 500), (2, 'wafa', 4000, 1000)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a571d57d-15a5-4257-8603-1782b0db8cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'maheer', 2000, 500), (2, 'wafa', 4000, 1000)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a7b66a4e-e50d-4845-9d0a-664a94ab8ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = rdd.toDF()\n",
    "\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "75b03610-e891-40e9-920f-a6618e999bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----+\n",
      "| _1|    _2|  _3|  _4|\n",
      "+---+------+----+----+\n",
      "|  1|maheer|2000| 500|\n",
      "|  2|  wafa|4000|1000|\n",
      "+---+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "471d6fd9-1cdc-44d6-b3a8-cd33c31610e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n",
      "| id|  name|salary|bonus|\n",
      "+---+------+------+-----+\n",
      "|  1|maheer|  2000|  500|\n",
      "|  2|  wafa|  4000| 1000|\n",
      "+---+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = rdd.toDF(schema = ['id', 'name', 'salary', 'bonus'])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c710f849-78c3-4615-bc10-1197e759d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n",
      "| id|  name|salary|bonus|\n",
      "+---+------+------+-----+\n",
      "|  1|maheer|  2000|  500|\n",
      "|  2|  wafa|  4000| 1000|\n",
      "+---+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame(rdd, schema = ['id', 'name', 'salary', 'bonus'])\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b86f5a-139c-4754-a27a-78b436e5043a",
   "metadata": {},
   "source": [
    "# map()\n",
    "\n",
    "- Its RDD transformation used to apply function (lambda) on every element of RDD and returns new RDD.\n",
    "- Dataframe doesn't have `map()` transformation to use with Dataframe you need to generate RDD first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f7f0ad74-6058-48d1-9b90-692823e8fec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maheer', 'shaik'), ('wafa', 'shaik')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('maheer', 'shaik'), ('wafa', 'shaik')]\n",
    "\n",
    "rdd = spark. sparkContext.parallelize (data)\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "deb7597e-3171-44ff-968c-e8f306f50552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maheer', 'shaik', 'maheer shaik'), ('wafa', 'shaik', 'wafa shaik')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newRDD = rdd.map(lambda x: x + (x[0]+' '+x[1],))\n",
    "\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2ecb780e-5bf0-42b0-ae1b-51a86c05abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| fname|lname|\n",
      "+------+-----+\n",
      "|maheer|shaik|\n",
      "|  wafa|shaik|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', 'shaik'), ('wafa', 'shaik')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['fname', 'lname'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55b38746-8f45-4e28-88e7-f8fed730987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------+\n",
      "| fname|lname|    fullname|\n",
      "+------+-----+------------+\n",
      "|maheer|shaik|maheer shaik|\n",
      "|  wafa|shaik|  wafa shaik|\n",
      "+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = df.rdd.map(lambda x: x + (x[0]+' '+x[1],))\n",
    "\n",
    "df = rdd1.toDF(schema=['fname', 'lname', 'fullname'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d776a9ff-9433-49a5-8f50-fc82a44694de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------+\n",
      "|    fn|   ln|    fullname|\n",
      "+------+-----+------------+\n",
      "|maheer|shaik|maheer shaik|\n",
      "|  wafa|shaik|  wafa shaik|\n",
      "+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fullname(x):\n",
    "    return x + (x[0]+' '+x[1],)\n",
    "    \n",
    "data = [('maheer', 'shaik'), ('wafa', 'shaik')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['fn', 'ln'])\n",
    "\n",
    "rddl = df.rdd.map(lambda x: fullname(x))\n",
    "\n",
    "df1 = rdd1.toDF(['fn', 'ln', ' fullname'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deec365-1aaa-4113-8125-82165dcd0692",
   "metadata": {},
   "source": [
    "# flatMap()\n",
    "\n",
    "- flatMap() is a transformation operation that flattens the RDD (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD\n",
    "- Its not available in dataframes. Explode() functions can be used in dataframes to flatten arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "04c2beac-efd7-439b-b882-9c32d35d0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maheer basha\n",
      "abdul wafa\n"
     ]
    }
   ],
   "source": [
    "data = ['maheer basha', 'abdul wafa']\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "for item in rdd.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6bb3e7cf-5db1-421e-b04e-4a41a8bbcf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maheer', 'basha']\n",
      "['abdul', 'wafa']\n"
     ]
    }
   ],
   "source": [
    "rddl = rdd.map(lambda x: x.split(' '))\n",
    "\n",
    "for item in rddl.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab3c7b64-1514-4cbd-a6e7-fdb3a0a97318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maheer\n",
      "basha\n",
      "abdul\n",
      "wafa\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "for item in rdd2.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98141ac0-b898-40ee-ba2d-463aad424302",
   "metadata": {},
   "source": [
    "# partitionBy()\n",
    "\n",
    "- Its used to partition large Dataset into smaller files based on one or multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ccebc0f-6da2-4fe3-8f8d-870d78733bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|  name|gender|dep|\n",
      "+---+------+------+---+\n",
      "|  1|maheer|  male| IT|\n",
      "|  2|  wafa|  male| HR|\n",
      "|  3|   asi|female| IT|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'male', 'IT'), (2, 'wafa', 'male', 'HR'), (3, 'asi', 'female', 'IT')]\n",
    "\n",
    "schema = ['id', 'name', 'gender', 'dep']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e79de77-10e0-4c1c-87eb-7c926bfa41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('./employees', mode='overwrite', partitionBy='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c9426f2c-8f26-4ab4-a000-d6728e2f6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|  name|gender|dep|\n",
      "+---+------+------+---+\n",
      "|  1|maheer|  male| IT|\n",
      "|  3|   asi|female| IT|\n",
      "|  2|  wafa|  male| HR|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('./employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5dd5d00d-567a-4abe-9504-dc1385b85ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|gender|\n",
      "+---+------+------+\n",
      "|  1|maheer|  male|\n",
      "|  3|   asi|female|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('./employees/dep=IT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4be02d83-ab78-4967-8f23-b9f0a9ff7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('./employees1', mode='overwrite', partitionBy=['dep', 'gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969ce72-da83-4e08-940d-08c50238f7b1",
   "metadata": {},
   "source": [
    "# from_json()\n",
    "\n",
    "- It's used to convert json string in to `MapType` or `StructType`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995dafd-79d5-4c7a-8641-48832823af3b",
   "metadata": {},
   "source": [
    "### MapType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a5ddbfea-6813-4981-abf5-3a53f2fa37c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|               props|\n",
      "+------+--------------------+\n",
      "|maheer|{\"hair\": \"black\",...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', '{\"hair\": \"black\", \"eye\": \"brown\"}')]\n",
    "schema = ['id', 'props']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "07c62825-2340-4eb8-81de-65f26e69afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cc24bcb8-dbd8-4335-87a1-7f3a613bde83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+-----------------------------+\n",
      "|id    |props                            |propsMap                     |\n",
      "+------+---------------------------------+-----------------------------+\n",
      "|maheer|{\"hair\": \"black\", \"eye\": \"brown\"}|{hair -> black, eye -> brown}|\n",
      "+------+---------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "mapTypeSchema = MapType(StringType(), StringType())\n",
    "\n",
    "df = df.withColumn('propsMap', from_json(df.props, mapTypeSchema))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "79961eba-456f-4bed-b54e-503e37a7ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      " |-- propsMap: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2ee91f0b-63ff-4b17-a9b6-49bfeb844794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+-----------------------------+-----+-----+\n",
      "|id    |props                            |propsMap                     |hair |eye  |\n",
      "+------+---------------------------------+-----------------------------+-----+-----+\n",
      "|maheer|{\"hair\": \"black\", \"eye\": \"brown\"}|{hair -> black, eye -> brown}|black|brown|\n",
      "+------+---------------------------------+-----------------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('hair', df.propsMap.hair).withColumn('eye', df.propsMap.eye)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c0a5e-99c3-4470-931d-59dcf2d031b1",
   "metadata": {},
   "source": [
    "### StructType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "719ea290-8827-4606-bfaa-3821eee26297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|               props|\n",
      "+------+--------------------+\n",
      "|maheer|{\"hair\": \"black\",...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', '{\"hair\": \"black\", \"eye\": \"brown\"}')]\n",
    "schema = ['id', 'props']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e247452-f149-4440-a695-a91c4909adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c4e56601-fa5a-4ed2-a180-cab39bbe3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------+--------------+\n",
      "|id    |props                            |propsStruct   |\n",
      "+------+---------------------------------+--------------+\n",
      "|maheer|{\"hair\": \"black\", \"eye\": \"brown\"}|{black, brown}|\n",
      "+------+---------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "structTypeSchema = StructType([StructField('hair', StringType()),\n",
    "                              StructField('eye', StringType())])\n",
    "\n",
    "df = df.withColumn('propsStruct', from_json(df.props, structTypeSchema))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "087c79c6-fe4c-4a2b-9b22-e650e185e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      " |-- propsStruct: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8e7313f1-3057-4507-8a83-ae46b80f186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+-----+-----+\n",
      "|    id|               props|   propsStruct| hair|  eye|\n",
      "+------+--------------------+--------------+-----+-----+\n",
      "|maheer|{\"hair\": \"black\",...|{black, brown}|black|brown|\n",
      "+------+--------------------+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('hair', df.propsStruct.hair).withColumn('eye', df.propsStruct.eye)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e588d6-1b7e-41bd-b31a-a1bd96766fcf",
   "metadata": {},
   "source": [
    "# to_json()\n",
    "\n",
    "- `to_json()` is used to convert DataFrame column `MapType` or `StructType` to JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f6a937fa-ff02-4ea4-a0f5-2b66875fb443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|          properties|\n",
      "+------+--------------------+\n",
      "|maheer|{eye -> brown, ha...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =[('maheer', {'hair': 'black', 'eye': 'brown'})]\n",
    "schema = ['name', 'properties']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "90c046b9-fa41-4b1a-a512-03b0242051e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a0144355-6691-42ab-b700-384d9d286188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+------------------------------+\n",
      "|name  |properties                   |propsString                   |\n",
      "+------+-----------------------------+------------------------------+\n",
      "|maheer|{eye -> brown, hair -> black}|{\"eye\":\"brown\",\"hair\":\"black\"}|\n",
      "+------+-----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "df = df.withColumn('propsString', to_json(df.properties))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "85e2318e-26e9-4bf0-a9ed-b76f72c91b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|    properties|\n",
      "+------+--------------+\n",
      "|maheer|{black, brown}|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =[('maheer', ('black', 'brown') )]\n",
    "schema = StructType([StructField ('name', StringType ()),\n",
    "                    StructField('properties', StructType([StructField('hair', StringType()), StructField('eye', StringType())]))])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a7794fd1-2ec1-422b-b4b6-5db2951649aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "29070b99-a0a4-4f9f-a295-95f62a877dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------------------------------+\n",
      "|name  |properties    |propsJSONString               |\n",
      "+------+--------------+------------------------------+\n",
      "|maheer|{black, brown}|{\"hair\":\"black\",\"eye\":\"brown\"}|\n",
      "+------+--------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('propsJSONString', to_json(df.properties))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd335383-21ff-4356-b463-c6b036695998",
   "metadata": {},
   "source": [
    "# json_tuple()\n",
    "\n",
    "- `json_tuple()` function is used to query or extract elements from json string column and create as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "75b57584-1383-47e9-b212-a9279eb49296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------+\n",
      "|name  |props                                             |\n",
      "+------+--------------------------------------------------+\n",
      "|maheer|{\"hair\": \"black\", \"eye\": \"brown\", \"skin\": \"brown\"}|\n",
      "|wafa  |{\"hair\": \"black\", \"eye\": \"blue\", \"skin\": \"white\"} |\n",
      "+------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', '{\"hair\": \"black\", \"eye\": \"brown\", \"skin\": \"brown\"}'),\n",
    "        ('wafa', '{\"hair\": \"black\", \"eye\": \"blue\", \"skin\": \"white\"}')]\n",
    "\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3e5667f1-2e59-4e8e-a782-9e9d61d9dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e9192f13-9a39-4b5b-8fd1-3ae59af267f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  name|   c0|   c1|\n",
      "+------+-----+-----+\n",
      "|maheer|brown|brown|\n",
      "|  wafa|white| blue|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df.select('name', json_tuple(df.props, 'skin', 'eye')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e4de0199-4133-4b79-aa97-c7f6bdec9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|  name|skin color|eye color|\n",
      "+------+----------+---------+\n",
      "|maheer|     brown|    brown|\n",
      "|  wafa|     white|     blue|\n",
      "+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', json_tuple(df.props, 'skin', 'eye').alias('skin color', 'eye color')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9887f-ce0c-4edc-9ec6-c99e489cb9ba",
   "metadata": {},
   "source": [
    "# get_json_object()\n",
    "\n",
    "- It's used to extract the JSON string based on path from the JSON column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cb110afe-4ecf-4176-add7-074bab3580c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------+\n",
      "|name  |props                                                                 |\n",
      "+------+----------------------------------------------------------------------+\n",
      "|maheer|{\"address\": {\"city\": \"hyd\", \"state\": \"telengana\"}, \"gender\": \"male\"}  |\n",
      "|wafa  |{\"address\": {\"city\": \"banglore\", \"state\": \"karnataka\"}, \"eye\": \"blue\"}|\n",
      "+------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', '{\"address\": {\"city\": \"hyd\", \"state\": \"telengana\"}, \"gender\": \"male\"}'),\n",
    "        ('wafa', '{\"address\": {\"city\": \"banglore\", \"state\": \"karnataka\"}, \"eye\": \"blue\"}')]\n",
    "schema = ['name', 'props']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "47da5770-9ea6-4dd2-800b-cd12f7eb5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- props: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "26e49e7e-2c71-46a0-9e07-42bbb0afe1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|name  |gender|\n",
      "+------+------+\n",
      "|maheer|male  |\n",
      "|wafa  |NULL  |\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df1 = df.select('name', get_json_object('props', '$.gender').alias('gender'))\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f122d24b-069e-461e-a4de-4239e701ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|    city|\n",
      "+------+--------+\n",
      "|maheer|     hyd|\n",
      "|  wafa|banglore|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select('name', get_json_object('props', '$.address.city').alias('city'))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7f25089e-27f2-4a5f-ad8d-212381706fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|  name|    city|    state|\n",
      "+------+--------+---------+\n",
      "|maheer|     hyd|telengana|\n",
      "|  wafa|banglore|karnataka|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select('name', get_json_object('props', '$.address.city').alias('city'),\n",
    "                           get_json_object('props', '$.address.state').alias('state'))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd212b-f613-469f-b82a-7ca776ed43e4",
   "metadata": {},
   "source": [
    "# Date: current_date(), to_date(), date_format()\n",
    "\n",
    "- DateType default fomart is `yyyy-MM-dd`.\n",
    "- `current_date()` get the current system date. By default, the data will be returned in `yyyy-dd-mm` format.\n",
    "- `date_format()` to parses the date and converts from `yyyy-MM-dd` to specified format.\n",
    "- `to_date()` converts date string in to datetype. We need to specify format of date in the string in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c4ca1821-98c0-4e2a-a76b-edb07f8189fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(5)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e01f3904-f958-410a-a787-416db1d324b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|current date|\n",
      "+---+------------+\n",
      "|  0|  2025-02-08|\n",
      "|  1|  2025-02-08|\n",
      "|  2|  2025-02-08|\n",
      "|  3|  2025-02-08|\n",
      "|  4|  2025-02-08|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df = df.withColumn('current date', current_date())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "90550b54-3694-426f-a85b-b8c16529390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current date: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "83c2287e-5d8b-4d96-9505-ab704c9d583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|current date|\n",
      "+---+------------+\n",
      "|  0|  08.02.2025|\n",
      "|  1|  08.02.2025|\n",
      "|  2|  08.02.2025|\n",
      "|  3|  08.02.2025|\n",
      "|  4|  08.02.2025|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df = df.withColumn('current date', date_format(df['current date'], 'dd.MM.yyyy'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "48a0d2a5-594b-4caa-860e-4f8a2c03c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current date: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9a8f0d4a-fbe0-4e45-b86e-a80ce265a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|current date|\n",
      "+---+------------+\n",
      "|  0|  2025-02-08|\n",
      "|  1|  2025-02-08|\n",
      "|  2|  2025-02-08|\n",
      "|  3|  2025-02-08|\n",
      "|  4|  2025-02-08|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn('current date', to_date(df['current date'], 'dd.MM.yyyy'))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# since date default type is yyyy-MM-dd so it display in same format here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d3fc3538-47ae-48e0-8c56-64634f59b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b2183-4e04-4a36-825a-7aa5b656dcc0",
   "metadata": {},
   "source": [
    "# datediff(), months_between(), add_months(), date_add(), month(), year()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e2300336-f023-41a3-9b4e-91f5998d3eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2015-06-14|2015-07-14|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-06-14', '2015-07-14')], ['d1','d2'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8f77f126-7394-4293-9d66-95182c364098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|        d1|        d2|date diff|\n",
      "+----------+----------+---------+\n",
      "|2015-06-14|2015-07-14|       30|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "df.withColumn('date diff', datediff(df.d2, df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "518a86ee-09a3-4582-b47c-13a9725c542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|month betn|\n",
      "+----------+----------+----------+\n",
      "|2015-06-14|2015-07-14|       1.0|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "df.withColumn('month betn', months_between(df.d2, df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4a1e9f6b-9f44-42fe-8f3c-420eac4f698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|        d1|        d2|added month|\n",
      "+----------+----------+-----------+\n",
      "|2015-06-14|2015-07-14| 2015-10-14|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "\n",
    "df.withColumn('added month', add_months(df.d2, 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "92208dcb-c5ba-42e0-a77c-56da52c5c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+\n",
      "|        d1|        d2|decreased month|\n",
      "+----------+----------+---------------+\n",
      "|2015-06-14|2015-07-14|     2015-04-14|\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('decreased month', add_months(df.d2, -3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "414e54ae-8e8e-4e08-a8bc-d98836b81dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|added days|\n",
      "+----------+----------+----------+\n",
      "|2015-06-14|2015-07-14|2015-07-21|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "df.withColumn('added days', date_add(df.d2, 7)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6c224dc5-e6e9-472b-a43a-9fbdebbc941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+\n",
      "|        d1|        d2|decreaed days|\n",
      "+----------+----------+-------------+\n",
      "|2015-06-14|2015-07-14|   2015-07-07|\n",
      "+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('decreaed days', date_add(df.d2, -7)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ab925d62-002b-4597-b054-6a4695e4e21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|        d1|        d2|year|\n",
      "+----------+----------+----+\n",
      "|2015-06-14|2015-07-14|2015|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df.withColumn('year', year(df.d2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "14fb45d9-c404-4054-966f-899f5b4669e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|        d1|        d2|month|\n",
      "+----------+----------+-----+\n",
      "|2015-06-14|2015-07-14|    7|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "df.withColumn('month', month(df.d2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962af9c-d500-4403-a984-8e307d39aa5d",
   "metadata": {},
   "source": [
    "# Timestamp: current_timestamp(), to_timestamp(), hour(), minute(), second()\n",
    "\n",
    "- TimestampType default fomart is `yyyy-MM-dd HH:mm:ss.SS`\n",
    "- `current_timestamp()` get the current timestamp. By default, the data will be returned in default format.\n",
    "- `to_timestamp()` converts timestamp string in to TimestampType. We need to specify format of timestamp in the string in the function.\n",
    "- `hour()`, `minute()`, `second()` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ce32ea0b-d042-4f6e-ad93-1f0910c7d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(7)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3bd7af32-3779-44c9-9b5e-e597572f579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |current timestamp      |\n",
      "+---+-----------------------+\n",
      "|0  |2025-02-08 14:25:02.676|\n",
      "|1  |2025-02-08 14:25:02.676|\n",
      "|2  |2025-02-08 14:25:02.676|\n",
      "|3  |2025-02-08 14:25:02.676|\n",
      "|4  |2025-02-08 14:25:02.676|\n",
      "|5  |2025-02-08 14:25:02.676|\n",
      "|6  |2025-02-08 14:25:02.676|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df = df.withColumn('current timestamp', current_timestamp())\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0a2a6544-1fd3-4697-828a-91e80b26e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "76760d87-8f4e-4aea-962b-df8fdb98a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+\n",
      "|id |current timestamp      |timestamp In String|\n",
      "+---+-----------------------+-------------------+\n",
      "|0  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|1  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|2  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|3  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|4  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|5  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "|6  |2025-02-08 14:25:02.788|12.25.2022 08.10.03|\n",
      "+---+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = df.withColumn('timestamp In String', lit('12.25.2022 08.10.03'))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0be22733-496e-4388-b89a-34fb42f5996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current timestamp: timestamp (nullable = false)\n",
      " |-- timestamp In String: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d42a8aa9-59b4-4520-b316-71bd44ede835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|   current timestamp|timestamp In String|\n",
      "+---+--------------------+-------------------+\n",
      "|  0|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  1|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  2|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  3|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  4|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  5|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "|  6|2025-02-08 14:25:...|2022-12-25 08:10:03|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn('timestamp In String', to_timestamp(df['timestamp In String'], 'MM.dd.yyyy HH.mm.ss'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d2147902-7689-482c-94f4-012dbebfecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- current timestamp: timestamp (nullable = false)\n",
      " |-- timestamp In String: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "95697b02-860e-46f0-90e3-f460467c9539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+----+--------+-------+\n",
      "|id |current timestamp      |timestamp In String|hour|minuutes|seconds|\n",
      "+---+-----------------------+-------------------+----+--------+-------+\n",
      "|0  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|1  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|2  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|3  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|4  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|5  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "|6  |2025-02-08 14:25:02.977|2022-12-25 08:10:03|14  |25      |2      |\n",
      "+---+-----------------------+-------------------+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, minute, second\n",
    "\n",
    "df.select('*', hour(df['current timestamp']).alias('hour'),\n",
    "                 minute(df['current timestamp']).alias('minuutes'),\n",
    "                 second(df['current timestamp']).alias('seconds'),).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bd05c-68fa-4cb3-92b8-97c7432963e4",
   "metadata": {},
   "source": [
    "# approx_count_distinct(), avg(), collect_list(), collect_set(), countDistinct(), count()\n",
    "\n",
    "Aggregate functions operate on a group of rows and calculate a single return value for every group.\n",
    "\n",
    "- `approx_count_distinct()` - returns the count of distinct items in a group of rows\n",
    "- `avg()` - returns average of values in a group of rows\n",
    "- `collect_list()` - returns all values from input column as list with duplicates\n",
    "- `collect_set()` - returns all values from input column as list without duplicates.\n",
    "- `countDistinct()` - returns number of distinct elements in input column.\n",
    "- `count()` - returns number of elements in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c8aeb86f-6d52-473e-a70d-123d69c2eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|       Maheer|        HR|  1500|\n",
      "|         Wafa|        IT|  3000|\n",
      "|          Asi|        HR|  1500|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"Maheer\", \"HR\", 1500),\n",
    "                (\"Wafa\", \"IT\", 3000),\n",
    "                (\"Asi\", \"HR\", 1500)]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d6ac3893-3c02-4a3a-ae0b-a6ffe22805de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            2|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5d688c5e-750c-499f-86b5-7bc96814a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     2000.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9fd1e94a-78f1-4a4c-ae66-4f3b74bb94b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|collect_list(salary)|\n",
      "+--------------------+\n",
      "|  [1500, 3000, 1500]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "df.select(collect_list('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "20dae817-e887-4e77-8402-85a0be446955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|collect_set(salary)|\n",
      "+-------------------+\n",
      "|       [3000, 1500]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "df.select(collect_set('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7153ac7e-449c-48d0-9a62-59fee5bc867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     2|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4787d8d4-837e-4bf5-8a89-820a94cd9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(salary)|\n",
      "+-------------+\n",
      "|            3|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6227cd-6874-40aa-a75e-916eb2509e92",
   "metadata": {},
   "source": [
    "# row_number(), rank(), dense_rank()\n",
    "\n",
    "we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause.\n",
    "\n",
    "- `row_number()` window function is used to give the sequential row number starting from 1 to the result of each window partition\n",
    "- `rank()` window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "- `dense_rank()` window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to `rank()` function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "23548f49-f01c-45a9-abdb-69b027d6f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+\n",
      "|    name|    dep|salary|\n",
      "+--------+-------+------+\n",
      "|  maheer|     HR|  2000|\n",
      "|bhargava|     HR|  2000|\n",
      "|     asi|     HR|  1500|\n",
      "|    wafa|     IT|  3000|\n",
      "|  martin|     IT|  2500|\n",
      "|  shakti|     IT|  3000|\n",
      "|Himanshu|     IT|  2000|\n",
      "| pradeep|     IT|  4000|\n",
      "|    annu|payroll|  3500|\n",
      "| karnthi|payroll|  2000|\n",
      "+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('maheer', 'HR' , 2000),\n",
    "        ( 'wafa', 'IT' , 3000),\n",
    "        ('asi', 'HR' ,1500),\n",
    "        ('annu', 'payroll',3500),\n",
    "        ('shakti', 'IT' ,3000),\n",
    "        ('pradeep', 'IT',4000),\n",
    "        ('karnthi', 'payroll' ,2000),\n",
    "        ('Himanshu', 'IT' ,2000),\n",
    "        ('bhargava', 'HR', 2000),\n",
    "       ('martin', 'IT', 2500)]\n",
    "schema = ['name', 'dep', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.sort('dep').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d4ec4b50-7fc1-4446-b92b-65301904ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('dep').orderBy('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "465676c2-5567-4037-a16d-e4d63c7c8429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+-------+\n",
      "|    name|    dep|salary|row no.|\n",
      "+--------+-------+------+-------+\n",
      "|     asi|     HR|  1500|      1|\n",
      "|  maheer|     HR|  2000|      2|\n",
      "|bhargava|     HR|  2000|      3|\n",
      "|Himanshu|     IT|  2000|      1|\n",
      "|  martin|     IT|  2500|      2|\n",
      "|    wafa|     IT|  3000|      3|\n",
      "|  shakti|     IT|  3000|      4|\n",
      "| pradeep|     IT|  4000|      5|\n",
      "| karnthi|payroll|  2000|      1|\n",
      "|    annu|payroll|  3500|      2|\n",
      "+--------+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df.withColumn('row no.', row_number().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "743ae30a-f94f-4e62-9dcf-ce910437e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----+\n",
      "|    name|    dep|salary|rank|\n",
      "+--------+-------+------+----+\n",
      "|     asi|     HR|  1500|   1|\n",
      "|  maheer|     HR|  2000|   2|\n",
      "|bhargava|     HR|  2000|   2|\n",
      "|Himanshu|     IT|  2000|   1|\n",
      "|  martin|     IT|  2500|   2|\n",
      "|    wafa|     IT|  3000|   3|\n",
      "|  shakti|     IT|  3000|   3|\n",
      "| pradeep|     IT|  4000|   5|\n",
      "| karnthi|payroll|  2000|   1|\n",
      "|    annu|payroll|  3500|   2|\n",
      "+--------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn('rank', rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "109a367a-e558-4c82-bb88-de2306aa1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|    name|    dep|salary|dense rank|\n",
      "+--------+-------+------+----------+\n",
      "|     asi|     HR|  1500|         1|\n",
      "|  maheer|     HR|  2000|         2|\n",
      "|bhargava|     HR|  2000|         2|\n",
      "|Himanshu|     IT|  2000|         1|\n",
      "|  martin|     IT|  2500|         2|\n",
      "|    wafa|     IT|  3000|         3|\n",
      "|  shakti|     IT|  3000|         3|\n",
      "| pradeep|     IT|  4000|         4|\n",
      "| karnthi|payroll|  2000|         1|\n",
      "|    annu|payroll|  3500|         2|\n",
      "+--------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn('dense rank', dense_rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baea162-4cb7-424e-8e8d-f0e7e319508d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2eb83-1252-4b58-9694-e03fedba9531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a81e5f-694f-44e6-b8c8-409dd9419a98",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc24766-dac2-43ac-af38-ccb99983dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
